{"0": {
    "doc": "I2 Course",
    "title": "I2 Intro Neuro/AI Course",
    "content": "Hello! Welcome to the Introduction to Neuro/AI Interactive Intelligence (i2) crash course (now opencourseware)! . Algorithm 1: Task Decider . you &lt;- role if you == student then continue reading else if you == instructor then read wiki end if . We hope you are excited to learn more about machine learning and its intersection with neuroscience. To get started, check out the Schedule. This is where you will have guided content week-to-week . This course relies heavily on the i2 Megadoc, which can be found here: i2 Megadoc . Please remember: . | We are all trying to learn, and learn at different speeds. | This course is somewhat of a commitment. You will get out what you put in | There are no grades! This is all for your learning alone. | Contact any mentor at any point if you have any questions. Seriously, we are here to help &lt;3 | Be helpful, be kind, and have fun! | . What basics you will learn (a more detailed list can be found in the wiki: . | ML | DL | Neuroanatomy | Computer Vision | Reinforcement Learning | Movement | Language Modeling | AI Fairness/Theory | Pain in RL/Neuroscience/Cognitive Science | More about the brain | . And also: . | Pytorch | Jupyter notebooks | LaTeX | Self-learning skills | . Go ahead and check out the Schedule! . ",
    "url": "/i2course/#i2-intro-neuroai-course",
    
    "relUrl": "/#i2-intro-neuroai-course"
  },"1": {
    "doc": "I2 Course",
    "title": "Also please check out the i2 Labs Website",
    "content": " ",
    "url": "/i2course/#also-please-check-out-the-i2-labs-website",
    
    "relUrl": "/#also-please-check-out-the-i2-labs-website"
  },"2": {
    "doc": "I2 Course",
    "title": "I2 Course",
    "content": " ",
    "url": "/i2course/",
    
    "relUrl": "/"
  },"3": {
    "doc": "Announcements",
    "title": "Announcements",
    "content": "Announcements are stored in the _announcements directory and rendered according to the layout file, _layouts/announcement.html. ",
    "url": "/i2course/announcements/",
    
    "relUrl": "/announcements/"
  },"4": {
    "doc": "Announcements",
    "title": "Site Creation!",
    "content": "Mar 26 &middot; 0 min read Welcome to a work in progress site for I2’s Intro to Neuro/AI Course! . ",
    "url": "/i2course/announcements/",
    
    "relUrl": "/announcements/"
  },"5": {
    "doc": "Megadoc",
    "title": "Megadoc",
    "content": "Hello, and welcome to the i2 Neuro/AI Megadoc. Here you can find all sorts of resources designed to help YOU become an amazing Neuro/AI* researcher! . How to use (ignore if following curriculum): . | Take a look at the table of contents and get a sense for what you do/don’t know . | Start from the beginning if you are brand new OR | Pick a topic that looks interesting! | . | For each major topic we have linked Resources, Questions, and sometimes a mini-project! | Be sure to ask questions and share cool things you learn with everyone in your group (if applicable)! | . This document assumes you have a basic understanding of Python, calculus, and linear algebra. Be sure to refresh concepts that you don’t know instead of continuing on without understanding. ",
    "url": "/i2course/megadoc/",
    
    "relUrl": "/megadoc/"
  },"6": {
    "doc": "Schedule",
    "title": "Schedule",
    "content": " ",
    "url": "/i2course/schedule/",
    
    "relUrl": "/schedule/"
  },"7": {
    "doc": "Schedule",
    "title": "Week 0: Course Overview, Prerequisite Knowledge",
    "content": "Purpose . | Gen acquainted with the course | Learn some of the pre-reqs necessary (Python) | Learn some of the pre-reqs necessary (calculus, linear algebra) | . Resources . Python Resources: . | CS50x Intro to Python Course | CS231n notes on Python, Numpy, and Jupyter Notebooks | . Math Resources: . | Linear Algebra Review and Reference (Just Unit 1) | Vector Basics | Hyperplane Definition | Hyperplane Math | Partial Derivatives (Mathematical) | Partial Derivatives (Graphical) | . Assignment . IMPORTANT!: If you already know a topic, DO NOT waste your time re-doing it. This week is just to refresh your mind . That being said, if you are unfamiliar with most of the items here, focus on Python and be sure to ask lots of questions! . Python Assignments: . | Complete units in the CS50x Intro to Python Course as needed (If you are confident, feel free to skip a section). I recommend: . | 0: Functions, Variables | 1: Conditionals | 2: Loops | 4: Libraries | 9: Et Cetera | . | Read through CS231n Notes, focusing on: . | Containers | Numpy | MatPlotLib | . | . Math Assignments: . | Read/Watch provided resources to learn/refresh your math knowledge. | If you are already familiar with the concepts listed above, feel free to skip them. I would, however, reccomend you explore linear algebra in high dimensional spaces (specifically hyperplanes) and revisit partial derivatives. | . Summary Questions . | What topics did you learn this week (Python or math)? | . ",
    "url": "/i2course/schedule/#week-0-course-overview-prerequisite-knowledge",
    
    "relUrl": "/schedule/#week-0-course-overview-prerequisite-knowledge"
  },"8": {
    "doc": "Schedule",
    "title": "Week 1: Introduction to Machine Learning",
    "content": "Purpose . An introduction to the foundation of modern AI! . | Introduction to Machine Learning (ML) | Math behind Linear Regression, SVM, PCA | Basic Unsupervised Clustering | . Resources . | I2 Resources Doc . | Unit 1 | . | . Assignment . | Complete Unit 1 in the megadoc (synthesis questions included) | Scikit-Learn ML Project (in megadoc/github) | . Summary Questions . | Synthesis questions in the megadoc | . ",
    "url": "/i2course/schedule/#week-1-introduction-to-machine-learning",
    
    "relUrl": "/schedule/#week-1-introduction-to-machine-learning"
  },"9": {
    "doc": "Schedule",
    "title": "Week 2: Introduction to Neural Networks",
    "content": "Purpose . Our first foray into more complex networks and how they learn. | Introduction to Deep Learning (DL) | Introduction to Neural Networks | Introduction to Backpropagation | . Resources . | I2 Resources Doc . | Unit 2 | . | CS231n Notes on Neural Networks. See Module 1: Neural Networks - great written resource for the basics. | . Assignment . | Complete Unit 2 in the megadoc (synthesis questions included) | Basic MNIST Classifier (in megadoc/github) | . Summary Questions . | Synthesis questions in the megadoc | . ",
    "url": "/i2course/schedule/#week-2-introduction-to-neural-networks",
    
    "relUrl": "/schedule/#week-2-introduction-to-neural-networks"
  },"10": {
    "doc": "Schedule",
    "title": "Week 3: Basic Neuroanatomy",
    "content": "Purpose . Welcome to a beginner’s introduction to neuroscience! We will . | Learn several basic regions of the brain | Learn fundamentals of the neuron and biological computation -Begin to hypothesize about the parallels and divergences of machine learning and the brain | . Resources . | I2 Resources Doc . | Unit 3, Basic Neuroanatomy | . | . Assignment . | Complete Unit 3 in the megadoc (synthesis questions included) | Basic Neuroanatomy Project (LaTeX template in github) | . Summary Questions . | Synthesis questions in the megadoc | . ",
    "url": "/i2course/schedule/#week-3-basic-neuroanatomy",
    
    "relUrl": "/schedule/#week-3-basic-neuroanatomy"
  },"11": {
    "doc": "Schedule",
    "title": "Week 4: Convolutional Neural Networks",
    "content": "Purpose . | Deep Learning for Vision | Learn about Convolutional Neural Networks | . Resources . | I2 Resources Doc . | Unit 5 | . | . Assignment . | Complete Unit 5 in the megadoc (synthesis questions included) | ConvNet Project (in megadoc/github) | . Summary Questions . | Synthesis questions in the megadoc | . ",
    "url": "/i2course/schedule/#week-4-convolutional-neural-networks",
    
    "relUrl": "/schedule/#week-4-convolutional-neural-networks"
  },"12": {
    "doc": "Schedule",
    "title": "Week 5: The Visual System",
    "content": "Purpose . This week we take a small break from computation and return to the brain, specifically your insanely complex and elegant visual system! We will . | Learn the basic anatomical and functional regions of the visual system | Compare biological solutions to visual tasks with computational solutions | . Resources . | I2 Resources Doc . | Unit 4 | . | . ",
    "url": "/i2course/schedule/#week-5-the-visual-system",
    
    "relUrl": "/schedule/#week-5-the-visual-system"
  },"13": {
    "doc": "Schedule",
    "title": "Week 6: Language Modeling",
    "content": "Purpose . | Learn about word embeddings | Learn about Recurrent Neural Networks | Learn about Transformers | Learn about fine-tuning foundation models like GPT | Gain a basic intuition about transformer and language models | . Resources . | I2 Resources Doc . | Unit 8 | . | . Assignment . | Complete Unit 8 in the megadoc (synthesis questions included) | HuggingFace Project Part 1 (in megadoc/github) | HuggingFace Project Part 2 (in megadoc/github) | . Summary Questions . | Synthesis questions in the megadoc | . ",
    "url": "/i2course/schedule/#week-6-language-modeling",
    
    "relUrl": "/schedule/#week-6-language-modeling"
  },"14": {
    "doc": "Schedule",
    "title": "Week 7: Introduction to Movement",
    "content": "Purpose . | Learn about movement in the context of the brain | . Resources . | I2 Resources Doc . | Unit 7 | . | . Assignment . | Complete Unit 7 in the megadoc (synthesis questions included) | Catch up on any work you have missed, since this unit is quite short | . Summary Questions . | Synthesis Questions in the megadoc | . ",
    "url": "/i2course/schedule/#week-7-introduction-to-movement",
    
    "relUrl": "/schedule/#week-7-introduction-to-movement"
  },"15": {
    "doc": "Schedule",
    "title": "Week 8: Reinforcement Learning",
    "content": "Purpose . | Go over basics of RL | Learn about Deep Q Learning | . Resources . | I2 Resources Doc . | Unit 6 | . | Spinning Up | . Assignment . | Complete Unit 6 in the megadoc (synthesis questions included) | Complete Unit 7 synthesis questions in the megadoc (OPTIONAL) | . Summary Questions . | Synthesis questions in the megadoc | What’s one resource that was helpful (suggested or found on your own)? | . ",
    "url": "/i2course/schedule/#week-8-reinforcement-learning",
    
    "relUrl": "/schedule/#week-8-reinforcement-learning"
  },"16": {
    "doc": "Schedule",
    "title": "Week 9: Building a Brain",
    "content": "Purpose . This week, we’re applying our knowledge. You’re building a brain! . | Understand the connectivity of the brain | Compare with the connectivity of analogous computation systems | Creatively improvise on existing methods of creating intelligent systems | . Resources . | I2 Resources Doc . | Unit 11 | . | . Assignment . | Complete Unit 11 in the megadoc (synthesis questions included) | Building a Brain Project (LaTeX template in github) | . Summary Questions . | Synthesis Questions in the megadoc | . ",
    "url": "/i2course/schedule/#week-9-building-a-brain",
    
    "relUrl": "/schedule/#week-9-building-a-brain"
  },"17": {
    "doc": "Staff",
    "title": "Staff",
    "content": "TODO: write happy intro about staff . TODO: add everyone else’s happy faces! . ",
    "url": "/i2course/staff/",
    
    "relUrl": "/staff/"
  },"18": {
    "doc": "Staff",
    "title": "Instructors",
    "content": "Varun Anath . ",
    "url": "/i2course/staff/#instructors",
    
    "relUrl": "/staff/#instructors"
  },"19": {
    "doc": "Staff",
    "title": "Teaching Assistants",
    "content": "Carter Swartout . swartout@uw.edu . Hi! . ",
    "url": "/i2course/staff/#teaching-assistants",
    
    "relUrl": "/staff/#teaching-assistants"
  },"20": {
    "doc": "Unit 01",
    "title": "Unit 1: The (machine learning) Basics",
    "content": "Hello and welcome to the Basics section of the I2 megadoc! The items here are fundamental building blocks for Deep Learning (powerful tools that are more complex in computation, but funnily enough not as technical). A lot of the things here are statistics-heavy so be sure to pay attention! We will start off with something I am sure most of you are familiar with - linear regression. Task: Read this article and answer the synthesis questions below! (15 min) . Before you begin! There are a few points in this article that I believe are more confusing than helpful, so feel free to ignore them if they do not make complete sense. Keep this in mind as you read, just take note, and don’t stress about the sections I overview below: . | In the subheader “Assumptions and caveats”, there is a bullet titled Output variable is a linear combination of feature variables — linearity. The author claims that linear regression can be used to find non-linear trends. He is right! The methods described check out and make sense. However, this is not what I would consider “simple linear regression” and can be considered a cool footnote. A computer will be handling most of this for you anyways. It is a good rule of thumb to apply linear regression models to linearly correlated data! | In the subheader “Assumptions and caveats”, there is a bullet titled Constant variance — homoscedasticity. There is a part towards the end that begins with the words “Practically speaking, we can account for this in one model without splitting out data into three groups…”. The intuition behind the author’s solution to the problem he posed earlier makes sense, but strays from simple linear regression once again. The intuition behind the problem of heteroscedasticity is valid however, so be sure to take note of that. | . Synthesis Questions: . | What is a feature in this context? | What are the significance of the β terms within the modified y = mx + b equation described in the article? | What is SSE? . | How is it calculated? | What can it tell you about the values you chose for β? | If you modify the β&lt;sub&gt;1 &lt;/sub&gt;term and the SSE goes up, was that a good modification? | . | Write out the linear regression formula (involving β) when you wish to estimate the impact of age, height, and weight of someone regarding their marital status. | Hint: How many β terms will there be? How many features? | . | What are the 4 assumptions described that you need to confirm before using linear regression? | What is homoscedasticity? What is heteroscedasticity? | . Also skim over this article and focus on the equation provided. No questions! . Something to note about ML in general: high-dimensional stuff is simply not visualizable as-is. If you have more than two features it’s near impossible to visualize the spread of dependent variables on a graph with the features as independent variables (How would you graph in 4d like we do in 2d or 3d?). Just know that equations of best fits for linear regressions define hyperplanes of the dimensions that the variables occupy. What is a hyperplane? Well, for example, if a space is 3-dimensional then its hyperplanes are the 2-dimensional planes, while if the space is 2-dimensional, its hyperplanes are the 1-dimensional lines. Hyperplanes are one dimension less than the space they “draw” through. Think about why it’s necessary to have a 2-d hyperplane for a 3d space of prediction (2 features) and a 1-d hyperplane for a 2d space of prediction (1 feature). While not visualizable, this reasoning applies to higher dimensions! Hyperplanes will become important as we move into dimensionality reduction so read up on them if you have time. The next topic to cover will be Support Vector Machines (SVMs). Task: Watch and understand the following videos. We recommend taking notes and being able to answer the synthesis questions provided below. Send your I2 teacher/mentor/overlord the answers to the questions over Discord. This first video is intuition only. Video 1: Support Vector Machines: Data Science Concepts (8 min) . Synthesis Questions: . | What are some use cases for an SVM? What does it do? | What is the margin? | Why does the margin need to be maximized? What does this allow for? | What are the support vectors? | What is the difference between a hard and soft margin SVM? | . This next video is math heavy. If you do not understand a term, look it up! Remember that the coefficients for a plane in the form (ax + by + cz = 0) can be found by determining a “normal” vector (a vector orthogonal to the plane). The vector will consist of numbers a, b, and c! This intuition carries over to higher dimensions (hyperplanes). Video 2: SVM (The Math): Data Science Concepts (10 min) . Synthesis Questions: . | wx - b = -1 defines a __________ . | Hint: it's like a plane | . | What are the \"w\" variables in the equation? | What is the equation for the decision boundary? | What is the size of the margin in terms of the vector w? | What needs to be minimized, and what are the constraints for finding the optimal hyperplane decision boundary? . | Hint: Near the end of the video | . | . So far you know about Linear Regression and SVMs! Take a moment to make sure you get the general idea of these concepts. We will now be moving into the idea of dimensionality reduction. As you may have noticed. A lot of what is done in the ML world is done in way more than three dimensions. Try as you might, you simply cannot accurately envision much above three dimensions concretely (Try and make a 4-d graph). We represent high-dimensional data in vectors, which is a nice numerical representation. But what if we want to see 100-dimensional data on a graph? This is where dimensionality reduction comes into play. We will cover a very basic dimensionality reduction algorithm. There are plenty more that have specific use cases so please** spend at least 10 minutes exploring others after learning about Principal Component Analysis (PCA)!** . Task: Watch and understand the following videos. We recommend taking notes and being able to answer the synthesis questions provided below. Send your I2 teacher/mentor/overlord the answers to the questions over Discord. There is a part in the video (20 seconds) that handles some pretty complex math. Feel free to ignore it. The idea is what we want you to learn. Not really the full math. Video 1: StatQuest: PCA main ideas in only 5 minutes!!! (5 min) . Video 2: Principle Component Analysis (PCA) (5 min) . Synthesis Questions: . | What does PCA do? . | Give 3 use cases that you can think of | . | What is a principal component? | . Project Spec: . The project for this “Basics” section will have you finish a code template on github. Please ask questions as you work through this project. Be sure to discuss with others in your group if you have one! Share your answers as you like, the goal is to learn and we’re not holding grades over your head. This project will be going over k-means clustering (unsupervised ML). We will be using the Scikit-Learn library. A few general helpful tips (if applicable): . | Use GitHub, it’s really just better | Use Anaconda with Python3 in VSCode. I personally create .py files but Jupyter Notebooks and Google Colab are also very powerful. | For a simple project like this though, powerful computing is unnecessary and you can figure out the details of those other technologies next week | If you use Anaconda, create a separate environment so you can mess with libraries and imports all day without screwing up your base environment. | . | Leave comments to cement your understanding. Link syntax to ideas. | . Check out this handy image that gives popular sk-learn clustering algorithms and their usages: . Also this image visualizing the clustering algorithms: . Read up on k-means clustering in the provided link (Images provided above also contained here). Feel free to check out the other algorithms as well: SK-Learn Clustering . Clone the Git repo onto your local device if you have not already. Then, in your local copy of the GitHub repo, navigate to the unit-1 folder, and work on clustering-pca.ipynb. Instructions are in the Jupyter notebook. If you need help setting up your python environment, ask the TA’s! . GH Link: Unit 1 Notebook (30 min) . When you are finished with your code, independently verify that it works and have fun with it! You could try this method on different datasets, such as this one for example. If you add any additional functionality be sure to talk about it with others and give them ideas. Remember that this is all for your learning, so do your best and don’t stress! . Congratulations! You now understand the basics of Clustering and PCA! . ",
    "url": "/i2course/megadoc/unit-01/#unit-1-the-machine-learning-basics",
    
    "relUrl": "/megadoc/unit-01/#unit-1-the-machine-learning-basics"
  },"21": {
    "doc": "Unit 01",
    "title": "Unit 01",
    "content": " ",
    "url": "/i2course/megadoc/unit-01/",
    
    "relUrl": "/megadoc/unit-01/"
  },"22": {
    "doc": "Unit 02",
    "title": "Unit 2: The (deep learning) Basics",
    "content": "Hello and welcome to the Basics section of the I2 megadoc! We will start by throwing a few videos at you that we believe give incredibly intuitive explanations of one of the foundational building blocks of modern Deep Learning. Task: Watch and understand the following videos. We recommend taking notes and being able to answer the synthesis questions provided below. Send your I2 teacher/mentor/overlord the answers to the questions over Discord. Video 1: But what is a neural network? | Chapter 1, Deep learning (20 min) . Synthesis Questions: . | What is a neuron (in terms of Neural Networks) and what does its \"activation\" represent? . | Bonus: Research and consider the correlation between a biological neuron and an artificial neuron. How are they similar/different? | . | What is a network layer? How is it connected to other network layers? | How is a picture of a digit decomposed into a network layer? | What does the final layer of a neural network represent? | What are weights? What are biases? Can you describe in English how information is passed from one layer to the next? | A neural network IS/IS NOT just a very highly parameterized function (Choose one) | What is the purpose of the sigmoid function? | . Great job! Now onto video two. Remember that all questions should be answered thoroughly and with a “because” if you can. Just to clarify some language: . Artificial Neural Network (ANN): A network as described in the video, but with just one hidden layer . Deep Neural Network: An ANN but with multiple hidden layers. When we say NN in this context, this is usually what we talk about. Fully Connected NN: Specifying that every single possible connection is made between adjacent layers (this is implicit to the networks shown in the video, but is not always the case!) . Video 2: Gradient descent, how neural networks learn | Chapter 2, Deep learning (20 min) . Synthesis Questions: . | Why is there a need for a train/test split for a neural network? Why is it important for a NN to be able to generalize to examples it has not seen? | Describe the Mean Squared Error (MSE) cost function. What does a higher value mean? What does a lower value mean? (For one training example) . | Bonus: Assume you have a binary classifier neural network that outputs the vector [0.25, 0.75] and you are using the MSE Loss function to train the Network. The data label indicates that the output for this training example should have been [0, 1]. What is the MSE Loss for this training example? | . | What is the gradient of a function? What is gradient descent? | What does minimizing the loss function do to the network's performance over time? | Do the hidden layers of a basic NN encode any useful information assuming you use the MSE Loss function? Why or why not? | . Great job! These last two videos definitely enter into more theoretical/difficult content, so be prepared. This is something you SHOULD have questions about, so post at least 1 in the Discord! . Video 3: What is backpropagation really doing? | Chapter 3, Deep learning (15 min) . Synthesis Question: . | Describe Gradient Descent, and how it works in principle with a Deep Neural Network. | . The last video may require understanding of partial derivatives (a MATH 126 concept) to fully understand. If you understand the basics of derivatives, however, this should not be too big a leap in understanding. There are no synthesis questions for this last video. Just watch and absorb! . Video 4: Backpropagation calculus | Chapter 4, Deep learning (10 min) . If you would like to jump into the math further, here is a longer lecture for JC I did last year walking through how backpropagation is done: . Part 1: Backpropagation, Part 1 | I2 JC . Part 2: Backpropagation, Part 2 | I2 JC . Project Spec: . The project for this “Basics” section will be following the tutorial below. Please ask questions in the discord as you work through this project. Be sure to discuss with others in your group! . A few helpful tips: . | Use GitHub, it’s really just better | Use Anaconda with Python3 in VSCode. I personally create .py files but Jupyter Notebooks and Google Colab are also very powerful. | For a simple project like this though, powerful computing is unnecessary and you can figure out the details of those other technologies next week | If you use Anaconda, create a separate environment so you can mess with libraries and imports all day without screwing up your base environment. | . | Leave comments to cement your understanding. Link syntax to ideas. | Read up on what MNIST is. | . Clone the Git repo onto your local device if you have not already. Then, in your local copy of the GitHub repo, navigate to the unit-2 folder, and work on mnist-dnn.ipynb. Instructions are in the Jupyter notebook. If you need help setting up your python environment, ask the TA’s! . GH Link: Unit 2 Notebook (1 hr) . When you are finished with your code, independently verify that it works and have fun with it! If you add any additional functionality be sure to talk about it with others and give them ideas. Remember that this is all for your learning, so do your best and don’t stress! . Congratulations! You now understand the basics of Deep Neural Network structure, how they learn, and how to create one using Python! . ",
    "url": "/i2course/megadoc/unit-02/#unit-2-the-deep-learning-basics",
    
    "relUrl": "/megadoc/unit-02/#unit-2-the-deep-learning-basics"
  },"23": {
    "doc": "Unit 02",
    "title": "Unit 02",
    "content": " ",
    "url": "/i2course/megadoc/unit-02/",
    
    "relUrl": "/megadoc/unit-02/"
  },"24": {
    "doc": "Unit 03",
    "title": "Unit 3: Basic Neuroanatomy",
    "content": "Welcome to the first lesson studying the most powerful and intelligent computer known to date – the brain. We will begin by studying what exactly it is. This is called neuroanatomy, and luckily for us, MIT OpenCourseware has a fabulous lecture on exactly that, which we will pair with some nice quick overviews from none other than Mr. Paul Andersen. Have a look! . Task 1: First, we will begin with Mr. Andersen’s lovely introduction to the structure of the foundation of our biological computation: the neuron! . The Neuron! . Synthesis Questions: . | Recall the main parts of a neuron and list their function | An action potential travels from the direction of the __, along the __, before reaching the __. | . Bonus Video: If you are interested in the gritty details of how computation works biologically, I suggest this video as a bonus: The Action Potential (the brain is like a salty banana!) . Task 2: Next, a crash course on the large modules of the brain before we go even deeper into the amazing functions of the brain. The Brain . Synthesis Questions: . | What distinguishes the forebrain, midbrain, and hindbrain? | What are several parts of the midbrain and what are their functions? | . Task 3: Watch and understand this beautiful lecture, then answer the synthesis questions provided below. 2. Neuroanatomy . Synthesis Questions: . | What are the four major parts of the brain? | Summarize the main functions of each of the four major components above, or jot down some details about each | What is a receptive field? | Describe characteristics of a cortical area. Find your favorite cortical area not described in the lecture and describe some things that make it interesting :) | . Congratulations! That was a lot of neuroscience! On to some more creative ways of learning in the project section. Project Spec: . There is no programming for this project. Instead, we have provided a LaTeX template for you to fill out. | If you are unaware of what LaTeX is, you can read about it here. | . GH Link: Unit 3 Template (30 min) . The questions in the template are also written below: . Unfortunately, we have neither brains nor creatures for you to dissect. However, for this project, we will be asking you to use your imagination and newfound biological and artificially-intelligible knowledge to . | Describe several advantages and disadvantages of biological computation with the brain compared to machine learning | Speculate what aspects of the architecture of the brain may cause these advantages or disadvantages, and similarly comment on aspects of machine learning’s architecture | Brainstorm some marvelous schemes for integrating advantages from both ways of computing. Draw, write, scribble etc… When you are done, do a quick google for your best ideas to see if anyone has researched or tried them already! | . Whatever you are able to conjure up, have something to show for it to demonstrate your knowledge about basic neuroanatomy! . ",
    "url": "/i2course/megadoc/unit-03/#unit-3-basic-neuroanatomy",
    
    "relUrl": "/megadoc/unit-03/#unit-3-basic-neuroanatomy"
  },"25": {
    "doc": "Unit 03",
    "title": "Unit 03",
    "content": " ",
    "url": "/i2course/megadoc/unit-03/",
    
    "relUrl": "/megadoc/unit-03/"
  },"26": {
    "doc": "Unit 04",
    "title": "Unit 4: Intro to the Visual System",
    "content": "Welcome to the visual system. Before we dive into how computer scientists have hacked together mathematical filters and matrix multiplication to process visual information and extract meaningful output, we will take a hard look at how your eyes and neurons process a continuous bombardment of photons. First, some vocab. Dorsal: Upper or back side of something (like a dorsal fin on an orca!) . Ventral: Underside or underbelly of something . Receptive Field: An input that produces the biggest response in some area or neuron is said to be its receptive field . Task 1: Watch the following videos and answer the questions! You know the drill. The first one will teach us about the physiology of processing light, the complex first layer of our own biological neural network. Vision: Crash Course Anatomy &amp; Physiology # 18 . Synthesis Questions: . | Where in the brain do signals from the retina go before reaching the visual cortex? | Describe the phenomena of retinal neurons that \"get tired\". Do you think there are analogous processes in deep learning or convolutional neural networks? | . The next video will describe the path of information flow through the visual cortex and some core properties of the structure of this process. Perception: 3.2 Primary Visual Cortex . Synthesis Questions: . | What wavelengths of light can humans detect? Why might we only be able to detect such a narrow band of light wavelengths? What would be an advantage and downside of processing more? | What is V1 in the visual cortex? What are those cells most sensitive to? | What is the fovea best at detecting? | Describe the retinotopic nature of the visual cortex (7:58) in your own words. Hypothesize whether convolutional neural networks might be organized as \"retinotopic\". | Describe cortical magnification and the causes of the trade-off between acuity and sensitivity. What are several reasons we can't see details from extremely far away, as for example, hawks can? | Brains are constrained by space, which as we have seen with vision, drives trade-offs in our processing. Do neural networks have analogous constraints? What are the effects of this? | . This final video gives a swift overview of many functional modules of visual processing. This is a great time to start thinking about how all this vision processing compares to our methods of processing information with neural networks! We will learn even more in the next Unit on convolutional neural networks too :) . Perception: 3.3 Functional Areas, Pathways, and Modules . Synthesis Questions: . | Describe at least 5 functional parts of the visual system | What is the \"what\" stream? What does it do? | What is the \"where\" stream? What does it do? | What is a region that processes faces? What inputs does it receive? What other regions is it near? | . Project Spec: . There is no programming for this project. Instead, we have provided a LaTeX template for you to fill out. | If you are unaware of what LaTeX is, you can read about it here. | . GH Link: Unit 4 Template (30 min) . The questions in the template are also written below: . Imagine you got a little too into neural networks and decided to replace your eyes with convolutional neural networks. You may use any sensors, hardware, brain computer interfaces, fungi, wires, Von Neumann computing, neuromorphic computing, or robots that you like (that seem vaguely feasible). How would you replace the algorithms run by the visual cortex with algorithms like those of convolution neural networks? . Draw your system in detail and write a short paragraph on the following: . | Why did you make the design decisions you made? | What would be the advantages of your system? | What would be the disadvantages? | What hardware did you use to implement this? In your opinion, is it possible to use the existing biological nervous system to run computation algorithms like CNNs? Why? | . ",
    "url": "/i2course/megadoc/unit-04/#unit-4-intro-to-the-visual-system",
    
    "relUrl": "/megadoc/unit-04/#unit-4-intro-to-the-visual-system"
  },"27": {
    "doc": "Unit 04",
    "title": "Unit 04",
    "content": " ",
    "url": "/i2course/megadoc/unit-04/",
    
    "relUrl": "/megadoc/unit-04/"
  },"28": {
    "doc": "Unit 05",
    "title": "Unit 5: Computer Vision",
    "content": "Hello and welcome to the Computer Vision (CV) section of the I2 megadoc! This section is, due to time constraints, only a very cursory glance at the foundations of CV. This is a whole subfield of ML and even if we spent 10 weeks on it, we wouldn’t scratch the surface! . Let’s start with some motivation. When you look at an image of (for example) a soda can, it does not matter where in the image the soda can is. You are able to detect it and know where it is. This detection ability is called translational invariance. You are able to detect an object even if it has been translated within an image. Traditional DNNs cannot do this (without being heavily overparameterized). Take a second to think about why the architecture of a DNN does not implicitly allow for translationally invariant object classification. | (Hint: Think about how different the vectorized images would be between the soda can image and its translated invariant. The inputs to the DNN would be drastically different and it would be hard to find any pattern!) | . Convolutional Neural Networks (or CNNs) solve this problem and much more. To understand what a CNN is though, you must first understand what a convolution is! . Task: Watch and understand the following videos. We recommend taking notes and being able to answer the synthesis questions provided below. Send your I2 teacher/mentor/overlord the answers to the questions over Discord. Watch up to 13:42 in the video, anything after that is extra information not needed for Deep Learning. Video: But what is a convolution? (13 min) . Synthesis Questions: . | What is the name for the smaller grid that convolves over a larger image? . | Hint: Starts with a \"k\" | . | What are some examples of what you can do to images if you convolve them with special matrices? | How does Gaussian blur \"work\"? | What is the name for the actual operation that occurs when the smaller grid is overlaid on the larger one? . | When each element of the corresponding pixels are multiplied then summed. | . | Give an example of a 3x3 matrix that would not do anything to the image it convolves over. Why does it not impact the image? . | This is also known as the \"do-nothing\" matrix | . | . Awesome job! Now we move onto integrating the concept of a convolution into a neural network. Task: Read the following article, watch the video, and answer the synthesis questions: . Article: Comprehensive CNN Guide (15 min) . Video: Visualizing Convolutional Neural Networks | Layer by Layer (5 min) . Synthesis Questions: . | The architecture of a CNN is loosely based on what part of the brain? | What is stride length? | What is padding? . | Why is padding useful? | . | What is the objective of the convolutional layer in a CNN? | What is the purpose of the pooling layer in a CNN? . | What are the two ways to pool shown to you in the article? | . | What is flattening and when is it done in a CNN? | What is the purpose of the feedforward layer in a CNN? | How do the convolutional layers before the feedforward layer in a CNN allow for higher accuracy? | . We have introduced you to the idea of a convolution and how convolutions are applied in CNNs. Can you begin to see how convolutions help with translational invariance? Think about it for a bit! Before the project, we just want to expose you to a few different types of convolutions. They aren’t all the same and serve different purposes. Task: Read the following article for the following sections: . | Convolution in Deep Learning | 3D Convolution | Transposed Convolution/Deconvolution | . Article: Comprehensive Convolution Types Guide) (15 min) . Awesome job! Feel free to move onto the project now. For those of you more interested in CV, there are a bunch more things to do in this sphere. Here are some topics you can explore independently: . | Transfer learning | Object detection (r-cnn, yolo) | Semantic segmentation (u-net, deeplab) | Self-supervised learning (colorization, damage correction, noise decoding) | Adversarial attacks | Image generation - variational autoencoders, generative adversarial networks | . Here are some slides from a JC on Variational Autoencoders (somewhat related, but very cool!): Variational Autoencoder JC Slides . Project Spec: . The project for this “Computer Vision” section will be following the tutorial/Jupyter Notebook below. Please ask questions in the discord as you work through this project. Be sure to discuss with others in your group! . A few helpful tips: . | Use GitHub, it’s really just better | Use Anaconda with Python3 in VSCode. | If you use Anaconda, create a separate environment so you can mess with libraries and imports all day without screwing up your base environment. | . | Type most of the code out yourself instead of just copying from the tutorial. | Leave comments to cement your understanding. Link syntax to ideas. | Read up on what Fashion-MNIST is (different than MNIST). | . Clone the Git repo onto your local device if you have not already. Then, in your local copy of the GitHub repo, navigate to the unit-5 folder, and work on conv-net.ipynb. Instructions are in the Jupyter notebook. If you need help setting up your python environment, ask the TA’s! . GH Link: Unit 5 Notebook** (1 hr)** . When you are finished with your code, independently verify that it works and have fun with it! If you add any additional functionality be sure to talk about it with others and give them ideas. Remember that this is all for your learning, so do your best and don’t stress! . Congratulations! You now understand the basics of Convolutional Neural networks! . ",
    "url": "/i2course/megadoc/unit-05/#unit-5-computer-vision",
    
    "relUrl": "/megadoc/unit-05/#unit-5-computer-vision"
  },"29": {
    "doc": "Unit 05",
    "title": "Unit 05",
    "content": " ",
    "url": "/i2course/megadoc/unit-05/",
    
    "relUrl": "/megadoc/unit-05/"
  },"30": {
    "doc": "Unit 06",
    "title": "Unit 6: Reinforcement Learning",
    "content": "Hello and welcome to the Reinforcement Learning (RL) section of the I2 megadoc! Reinforcement learning is an incredibly powerful subfield of Machine Learning that can be used to deploy intelligent autonomous agents in both real and simulated systems. Reinforcement learning in itself is not inherently tied to AI. It is its own field that has strong ties to probability and statistics theory. For the purposes of simplicity, we will only cover the very basics and terminology of Deep RL (RL combined with deep learning). Just as a heads up, RL is probably one of the most “deep” and currently relevant subfields of ML, followed closely by language modeling (this may also be the Dunning-Kruger effect, RL is what I have the most expertise on). Folks at OpenAI have compiled a plethora of resources for Deep RL and Deep RL research. We will show you the most relevant articles they have published but they have a wealth of knowledge to learn! Feel free to poke around on the Spinning Up page in your own time. Task: Read the Key Concepts in RL section of the Spinning Up Intro to RL page. This is a short, but dense article. (45 min) . You may skip the section on Diagonal Gaussian Policies. Synthesis Questions: . | In what situations do we use reinforcement learning? What kinds of problems does it solve? . | Give an example of RL used to play a game. Did it outperform humans? Does this scare you? | . | What do the variables s, a, τ, represent in RL? . | What do s', a' represent? | . | What is an action space? | What is an episode? | How is a neural network used to generate a probability distribution over actions given a state? . | (i.e How would you construct a network if the dimensionality of the state was n1 and the action space had a cardinality of m1) | How is the log probability calculated from the logits of the NN output? | Note: This is called an actor NN! | . | Describe infinite-horizon discounted return and what the discount factor is | Describe what a stochastic policy is (you may have to look up what stochasticity is), and what it means for a policy to be parameterized by θ | What is a good policy looking to maximize? . | Think: If you set up your rewards in your environment randomly, would an RL model learn anything? No. This ties into the greater idea of reward-shaping, which is the concept of how to place rewards in an environment to encourage \"good\" behavior by an RL agent. | . | What is the difference between the value function and the Q-function? . | Describe the connection between the two | . | Based on the idea behind Bellman Equations (\"The value of your starting point is the reward you expect to get from being there, plus the value of wherever you land next.\") Explain how the following two equations satisfy the idea behind Bellman Equations: | . This may help: . | What is the advantage function? | . This was a lot of content. Good job on getting through it all! Reinforcement learning is quite a math heavy subfield. Get used to a whole lot of probability and integration. Spelling out exactly what the equations are doing (by reading them out loud) can help quite a bit. Next we will be diving into some in-practice algorithms on a high level. So, in general, these equations that you have learned about are ideals, not what is done in practice. What generally happens with deep learning RL algorithms is that these functions are **approximated **by a neural network rather than calculated. This is important to keep in mind as you go on, because a lot of these equations have some recursiveness to them and often have such a huge search space that finding the perfect parameters for V* or Q* is near impossible in complex environments. Task: Read the “Kinds of RL Algorithms” section of the Spinning Up Intro to RL page.** (15 min)** . You may skip the section on What to Learn in Model-Based RL . Synthesis Questions: . | What is the difference between model-free and model-based RL? | What is the difference between policy approximators and Q-learning? | Think back to what the J([π](https://www.pisymbol.net/#:~:text=Alt%2B960%20Press%20and%20hold,enter%20960%20on%20numeric%20keypad.)) function was. How would performing gradient ascent (finding where this function is locally highest) on this function help the agent perform well in its environment? | How is a policy derived (not literally a derivation) from the Q-function? Hint: argmax | . Great job on completing the tasks! . The last thing I will talk about before the project is the concept of epsilon-greediness for Q-learning. In essence, argmaxing the Q-function can lead to being stuck in a “pocket” where the agent gets stuck taking the same few trajectories due to the deterministic selection of the action to take. Since Q-learning is usually model-free, there is no way to “see” what lies beyond what is explored. Since the model will not take the actions to explore these trajectories, possibly more efficient solutions cannot be stumbled on by chance. This is where epsilon greediness comes in. With probability ε, the policy will take a random action rather than the one that maximizes the Q-function. Ε starts off large and diminishes over time, as the model is expected to have learned better by then and there is less need to explore rather than exploit the learned model for high returns. You can see how epsilon changes through episodes in the graph below. You can read more about epsilon-greedy algorithms in this article (Section 5.2). It works quite well and often gets a model “unstuck” from optimizing poorly through not exploring. Project Spec: . The project for this “Reinforcement Learning” section will be following the tutorial/Jupyter Notebook below. Please ask questions in the discord as you work through this project. Be sure to discuss with others in your group! . A few helpful tips: . | Use GitHub, it’s really just better | Use Anaconda with Python3 in VSCode. | If you use Anaconda, create a separate environment so you can mess with libraries and imports all day without screwing up your base environment. | . | Type most of the code out yourself instead of just copying from the tutorial. | Leave comments to cement your understanding. Link syntax to ideas. | . Then, in your local copy of the GitHub repo, navigate to the unit-6 folder, and work on rl_net.ipynb. Instructions are in the Jupyter notebook. If you need help setting up your python environment, ask the TA’s! . GH Link: Unit 6 Notebook (1.5 hr) . | In case there is an error with the notebook not showing up on GitHub, paste the link into this website to view it. | . When you are finished with your code, independently verify that it works and have fun with it! If you add any additional functionality be sure to talk about it with others and give them ideas. Congratulations! You now understand the (incredibly basic) basics of Deep RL! . ",
    "url": "/i2course/megadoc/unit-06/#unit-6-reinforcement-learning",
    
    "relUrl": "/megadoc/unit-06/#unit-6-reinforcement-learning"
  },"31": {
    "doc": "Unit 06",
    "title": "Unit 06",
    "content": " ",
    "url": "/i2course/megadoc/unit-06/",
    
    "relUrl": "/megadoc/unit-06/"
  },"32": {
    "doc": "Unit 07",
    "title": "Unit 7: Intro to Movement",
    "content": "We are now going to be learning about how your brain makes you act and move in your environment. We start by investigating the cerebellum. This will tie into Reinforcement Learning (RL), so pay attention! . First, a few useful vocab words: . Afferent: Conducting towards something . Efferent: Conducting away from something . Task 1: Read the sections entitled “Feed forward control systems” and “The cerebellum may be a feedforward control system” from the notes here to get a sense for the purpose of the cerebellum. Cerebellum – The University of Texas Medical School . Then, watch the following brief videos, take notes if you’d like! . 2-Minute Neuroscience: Cerebellum . 19.3 Cerebellar Information Flow . Synthesis Questions: . | Explain the difference between mossy fibers and climbing fibers and their respective functions. | Which cells or fibers may be responsible for sending error signals to the cerebellum? | What is the function of a Purkinje cell? To what do they output? What do they look like? Why might they look like this? | What are the primary input sources of the cerebellum? Where do these signals originate? | . For more depth on these topics, here are some more fabulous resources. Bonus Videos: . 19.4 Cerebellar Circuits . 19.5 Afferent Tracts to the Cerebellum . ",
    "url": "/i2course/megadoc/unit-07/#unit-7-intro-to-movement",
    
    "relUrl": "/megadoc/unit-07/#unit-7-intro-to-movement"
  },"33": {
    "doc": "Unit 07",
    "title": "Unit 07",
    "content": " ",
    "url": "/i2course/megadoc/unit-07/",
    
    "relUrl": "/megadoc/unit-07/"
  },"34": {
    "doc": "Unit 08",
    "title": "Unit 8: Language Modeling",
    "content": "Hello and welcome to the Language Modeling (LM) section of the I2 megadoc! Language modeling is an incredibly pertinent field of deep learning that is focused on using statistical modeling to generate sequences of language tokens (I made this term very general on purpose). This can mean Q/A prompts/output like ChatGPT, Seq2Seq models for machine translation, or sentiment analysis on text messages. Language Modeling, like Reinforcement Learning, is an incredibly large subfield that we simply cannot do justice in 1 unit. We will, however, try to introduce you to some basic language Modeling Deep Learning architectures that get more and more complex. We will also introduce you to HuggingFace, a service that allows you to learn about, train, load, finetune, and save DL models. First, we will explore the concept of word embeddings. Understanding this is crucial to clearing up some FAQ about Language Modeling. So far most of the data you have worked with has been easily “castable” into n-dimensional vectors. However, when we are working with language, how can we convert a word like “King” into a vector? Performing computation directly on words is an incredibly difficult task. However, if we can “encode” the meaning of the word “King” into a vector consisting of numbers and have some way of converting between the two then the problem becomes easier. Task: Read this “Word Embeddings” article and answer the following synthesis questions: . You may skip the “Recursive Neural Networks” subsection of the article if you wish . Synthesis Questions: . | What is a word embedding? How long are they usually? | How does training a network to recognize the validity of 5-grams result in a Word-to-Vector \"map\"? . | Can you think of another training method to achieve the same side-effect? | . | Pretend you are a word embedder. Give examples (2-3 for each) of words in the same family as: . | King | Button | Pain | Water Bottle | . | What is the explanation given for the emergence of the \"male-female difference vector\"? | What is pre training/transfer learning/multi-task learning? | . Now we go into Recurrent Neural Networks (RNN’s) and the concept of Backpropagation Through Time (BPTT) and its drawbacks. This builds off of Unit 2 (Deep learning) so feel free to revisit those if you are having trouble! . Task: Read this “How RNN’s Work” article and answer the following synthesis questions: . You may skip the “Word Embedding” and “Backward Pass” subsections of the article if you wish. There is a better resource below that explains the concept of a Backwards Pass in RNNs (BPTT) . Helpful clarifications for after you read: . | Note that what is “inside” the RNN unit (the circles on the computational graph) is modifiable, and I have attached here a simple look into the inside: | . | I also have attached an optional reading about Gated Recurrent Units (GRU’s) here. “GRUs have the ability to keep memory/state from previous activations rather than replacing the entire activation like a vanilla RNN, allowing them to remember features for a long time and allowing backpropagation to happen through multiple bounded nonlinearities, which reduces the likelihood of the vanishing gradient” (GRUs vs LSTMs). | . Synthesis Questions: . | What does a language model, in general, try to predict? . | Hint: Predicting X given Y. What are X and Y? | . | What happens to the memory vector as we move through time? | Describe how a RNN would deal with the sentence \"How are you?\" in terms of its unrolled computational graph . | Basically, what happens to these words and the hidden states generated from these words? | How long would the unrolled computational graph be in terms of RNN nodes (circles)? | . | What is the memory vector initialized to? | What is &amp;lt;\\s&gt;? What does it signify according to the article? | . Task: Read this “Backpropagation Through Time” (BPTT) article, a small extension: “Truncated BPTT” (read only section 2.8.6) and answer the following synthesis questions: . Synthesis Questions: . TODO: what are the sub tags? . | What are W&lt;sub&gt;x&lt;/sub&gt;, W&lt;sub&gt;y&lt;/sub&gt;, and W&lt;sub&gt;s&lt;/sub&gt;? | Why does BPTT not work with a large number of timesteps? . | What is this problem called? | . | How does Truncated BPTT solve this problem? | . Finally we get into the Transformer architecture, which as of 2023 seems to have a grip on the DL market as the most generally powerful type of neural network used in all sorts of LM tasks. This subsection is especially difficult. Please read the articles slowly and carefully. Be sure to ask for help if you have any questions or trouble understanding a statement! . Task: Read the following “Intuitive Explanation of GPT Models” Part 1 and Part 2 (and hopefully part 3 soon!) (Made by University of Washington Interactive Intelligence member Carter!) and answer the following synthesis questions: . In case this article is not enough, or you find yourself struggling to fully understand transformers, take a look at this list of other links that can reinforce your knowledge: . | The Illustrated Transformer Another good explanation of Transformers. | GPT in 60 lines of NumPy - explains GPT in a concise way using code! | The Annotated Transformer - the Attention Is All You Need paper annotated with PyTorch. | NanoGPT, simple example code for a GPT model. | . Synthesis Questions: . | Why does not being able to capture long-term dependencies result in nonsensical generated paragraphs (like clicking autocomplete)? | What do the probabilities that GPT outputs represent, and what is greedy decoding? | What is a token? | What are the big blocks that make up the GPT architecture? | What are the two main blocks inside a transformer-decoder block? | Describe masked self-attention in your own words (not including the vector math). | How would this help stop generated sequences from being mostly nonsensical? | . | How are Query, Key, and Value vectors generated from each word embedding? | How is the score for each word calculated using one word's query vector and all the other words' key vectors? | How is the score for each word and its value vector used to create the vector for a single transformed word? (In the case of the article, the word is \"He\"). | Briefly describe multi-headed attention | . Awesome job answering those synthesis questions for Transformers! Now we move onto the project (which won’t be nearly as difficult) . Project Spec: . The project for this “Language Modeling” section will be following the tutorial/Jupyter Notebook below. Please ask questions in the discord as you work through this project. Be sure to discuss with others in your group! . A few helpful tips: . | Use GitHub, it’s really just better | Use Anaconda with Python3 in VSCode. | If you use Anaconda, create a separate environment so you can mess with libraries and imports all day without screwing up your base environment. | . | Leave comments to cement your understanding. Link syntax to ideas. | . Clone the Git repo onto your local device if you have not already. There are 2 parts (.ipynb files) to this unit. Finish both. In your local copy of the GitHub repo, navigate to the unit-8 folder, and work on hf_tutorial.ipynb. Instructions are in the Jupyter notebook. If you need help setting up your python environment, ask the TA’s! . GH Link: Unit 8 Notebook Part 1** (1 hr)** . Now navigate to the application portion of this project, where you are given a dataset and asked to train an LLM of your choice to emulate Shakespeare! Be sure to reference your Unit 8 Notebook Part 1 to figure out how to do this. The starter code is in lm_starter_code.ipynb . GH Link: Unit 8 Notebook Part 2 (1 hr) . When you are finished with your code, independently verify that it works and have fun with it! If you add any additional functionality be sure to talk about it with others and give them ideas. Congratulations! You now understand the basics of Language Modeling! . ",
    "url": "/i2course/megadoc/unit-08/#unit-8-language-modeling",
    
    "relUrl": "/megadoc/unit-08/#unit-8-language-modeling"
  },"35": {
    "doc": "Unit 08",
    "title": "Unit 08",
    "content": " ",
    "url": "/i2course/megadoc/unit-08/",
    
    "relUrl": "/megadoc/unit-08/"
  },"36": {
    "doc": "Unit 09",
    "title": "Unit 9: Fairness and Theory",
    "content": "TODO: format . Machine learning models are interesting, but they have substantive effects on the world they are deployed in. How can we make these models fairer, safer, less biased, and/or more responsible? Is this even rigorously possible? (Some researchers suggest not!) What is the source of bias? (“Garbage In, Garbage Out” is a stunted answer, and maybe even misleading.) These are all questions which are intimately linked with deep learning theory, a growing field which attempts to explain how neural networks work rather than attempting to advance the SOTA in performance or a similar task. Because of the close relationship between theory and fairness research, we will be exploring them together. After going through this unit, you will be able to reason about deep learning at a very abstract level (a powerful tool for research and experimentation); identify the core theoretical essence of various models and approaches; think critically about what the concepts of ‘bias’, ‘fairness’, ‘robustness’, ‘responsibility’, and ‘fairness’ mean and how we might build models which better embody these values. It is recommended to read the listed papers in order, and to at least skim each one. Theory . | Universal Approximation Theorem. While it’s not necessary to completely understand the proof, make sure you understand at least what the theorem is stating and why it is an interesting result. | Read this introductory article written by Andre (the author of this unit) on the UAT, then this Twitter thread of Yann Lecun blasting it. Then, read Lecun et al.’s paper Learning in High Dimension Always Amounts to Extrapolation. Lastly, read this document of the debate on Twitter. Now think about what your position in this debate is. What is interpolation? What is extrapolation? Do neural networks extrapolate? Is this a meaningful concept at all, and if not, what might be a more meaningful one? Keep thinking about these questions throughout the theory section. | Deep Double Descent: Where Bigger Models and More Data Hurt. A ‘classic’ empirical finding which points towards a weirdness of deep learning models as opposed to less parametrized, classical models. | Are Deep Neural Networks Dramatically Overfitted? A great technical blog post giving more theory on the question of overfitting. | Understanding Deep Learning Requires Rethinking Generalization. Important empirical results and speculative theoretical work. | Methods for Pruning Deep Neural Networks. You can skim this one, but it’s a good coverage of pruning – an empirical method whose success is surprising and is worth thinking about. | The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks. This is our second major theory paper. What explains the results found and answers the questions raised in 2, 3, 4, and 5? The Lottery Ticket Hypothesis is a compelling theory. | Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask. A further investigation into the Lottery Ticket Hypothesis. | What’s Hidden in a Randomly Weighted Neural Network? A fascinating result from the Lottery Ticket Hypothesis. | Neural Tangent Kernel. Choose at least one of these to read. | https://lilianweng.github.io/posts/2022-09-08-ntk/ | https://blog.ml.cmu.edu/2019/10/03/ultra-wide-deep-nets-and-the-neural-tangent-kernel-ntk/ | https://rajatvd.github.io/NTK/ | https://www.inference.vc/neural-tangent-kernels-some-intuition-for-kernel-gradient-descent/ | . | The Modern Mathematics of Deep Learning. A landmark work in developing a mathematical theory of deep learning. Skim sections 2.3, 3.2, 3.3, and 4. Make sure you at least understand the results at a high level. | Language Models (Mostly) Know What They Know. A theoretical method for probing language model knowledge reveals interesting epistemic structures. | Bonus: A Mathematical Framework for Transformer Circuits. Think you know how transformers work? Think again! | . Fairness, Responsibility, Safety . | A Survey on Bias and Fairness in Machine Learning. A good and comprehensive survey of general concerns and approaches to addressing bias in machine learning problems. | On the (im)possibility of fairness. No need to read it too in detail; skimming it and understanding the main result is fine. Argues that different mathematized components of algorithmic fairness are fundamentally incompatible with each other in the ideal. | The Myth in the Methodology: Towards a Recontextualization of Fairness in Machine Learning. We’ve got it all wrong, philosopher Lily Hu shows us. Fairness cannot be mathematized. | What’s sex got to do with fair machine learning? An investigation of gender variables in machine learning models. | What is ‘race’ in algorithmic discrimination on the basis of race? An investigation of race variables in machine learning methods. | Moving beyond “algorithmic bias is a data problem”. Sara Hooker takes on the pervasive idea of GIGO (Garbage In, Garbage Out) suggests that data is the root source of bias. | What Do Compressed Deep Neural Networks Forget? An empirical follow-up from the previous opinion piece. How do different compression methods affect model performance? . | Optional: Characterizing and Mitigating Bias in Compact Models. Related work if you are interested in additional research in this direction. | . | The Curious Case of Common Sense. Our very own professor Yejin Choi reflects on the difficulty of codifying common sense into AI models. | You may be interested in Yejin Choi’s research papers, each of which address different dimensions of common sense reasoning: https://homes.cs.washington.edu/~yejin/ | . | Can Machines learn Morality? The Delphi Experiment. An attempt to train a language model to understand commonsense morality. You can play with the Delphi model at https://delphi.allenai.org/. | Constitutional AI: Harmlessness from AI Feedback. An important proposal for a framework to regulate AI with AI from a set of constitutional principles. | Predictability and Surprise in Large Generative Models. An empirical and theoretical investigation of the difficulties of regulating LLMs. | The Hardware Lottery. Thinking about how AI development is constrained by the hardware available. | Socially situated artificial intelligence enables learning from human interaction. A very interesting paper from our very own professor Ranjay Krishna on how humans can more actively engage with and shape AI models. | On the Opportunities and Risks of Foundation Models. A classic in AI safety. Read the introduction (section 1) and “Society” (section 5) at minimum. | The Dark Side of Techno-Utopianism. An accessible and thoughtful conclusion to this unit: what is our role in tech, and is it as rosy as we think it is? | . Task . Read the GPT-3 paper, the GPT-4 model card, or the LaMDA paper (or another recent large language model paper). Focus on the discussions on safety and fairness. Drawing upon the sources in both the theory and the fairness sections of this unit, criticize the evaluations – identify what is mistaken, what assumptions are made, what is socially problematic, what possible biases may have been unidentified or which lie latent in the design, etc. Feel free to talk with Andre if you have any questions while reading or thinking about these topics. If you are interested in a more philosophical investigation of these topics, check in the machine subjectivity discord channel or join one of our meetings! . ",
    "url": "/i2course/megadoc/unit-09/#unit-9-fairness-and-theory",
    
    "relUrl": "/megadoc/unit-09/#unit-9-fairness-and-theory"
  },"37": {
    "doc": "Unit 09",
    "title": "Unit 09",
    "content": " ",
    "url": "/i2course/megadoc/unit-09/",
    
    "relUrl": "/megadoc/unit-09/"
  },"38": {
    "doc": "Unit 10",
    "title": "Unit 10: Cognitive Science and Psychology of Reward and Pain",
    "content": "Task: This unit is very simple. Read our paper! (1 hr) . Paper: Deinforcement Learning v2 . Post your thoughts/questions if you have any! Be sure to review Unit 6 if you need a refresher on Reinforcement Learning. ",
    "url": "/i2course/megadoc/unit-10/#unit-10-cognitive-science-and-psychology-of-reward-and-pain",
    
    "relUrl": "/megadoc/unit-10/#unit-10-cognitive-science-and-psychology-of-reward-and-pain"
  },"39": {
    "doc": "Unit 10",
    "title": "Unit 10",
    "content": " ",
    "url": "/i2course/megadoc/unit-10/",
    
    "relUrl": "/megadoc/unit-10/"
  },"40": {
    "doc": "Unit 11",
    "title": "Unit 11: Building a Brain: Networks and Systems",
    "content": "It would be negligent not to begin by acknowledging the inadequacy of this chapter. The connectivity and wiring of the brain as well as the genetics and epigenetics that comprise it are far from fully understood, and these resources are merely scratching the surface of a profound ocean of literature and undiscovered territory of building the systems of a mind. Nonetheless, we begin with a lecture investigating the genetic and learned nature of human behaviors. Task 1: 10. Development, Nature &amp; Nurture I . Synthesis Questions: . | What is one possible mechanism for building knowledge from parts of the brain present at birth? What kinds of knowledge might be innate and \"precursory\"? . | Why might it be useful for those precursors of knowledge to be innate rather than learned? | Google and find 1-2 other possible rudimentary or innate processes in the brain. Do any machine learning techniques use similar innate predispositions or ingrained knowledge? | . | What is perceptual narrowing? Are there analogous processes, protocols, or side effects in machine learning? What are possible downsides and upsides of the effect? . | What might you wish you could learn faster, and what might be a cost to having neural architectures that support that kind of learning? | . | If humans are uniquely suited to recognizing faces, what might be a useful analogous skill in machine learning which may support more human-like behavior? | . **Task 2: **Next, briefly skim the questions below then indulge in this scintillating lecture describing brain modules and the discovery of their connected wiring! . Start watching at 8:40, 21. Brain Networks . For your reference, a voxel is a 3D pixel. Think of it as a minecraft block of the brain. Also a term used in computer graphics. Synthesis Questions: . | What is white matter? Why does it matter? | What is a connectivity fingerprint? . | Why might we care about the connectivity similarities of other species and what implications might findings in this research have? | . | Describe three large connections between major regions in the brain and what those regions do, using Google if necessary. | What is the default mode network? What purpose is it hypothesized to serve? . | If you were to build an analog of the DMN in a machine learning network placed in your favorite video game as an environment, what function would you design it to serve? | . | Hypothesize how multiple demand regions might work and why they might be necessary. I don't know the right answer here so go nuts. | . Project Spec: . There is no programming for this project. Instead, we have provided a LaTeX template for you to fill out. | If you are unaware of what LaTeX is, you can read about it here. | . GH Link: Unit 11 Template (40 min) . The questions in the template are also written below: . Ohhh this one’s good. I’m tapping my fingers together like Dr. Evil. You are designing an egg. It will become a lifeform on a new planet. The planet is almost completely covered in water. Small islands made from underwater volcanoes dot the surface. The gravity is stronger on this planet and the biodiversity is lower than on Earth. There is less direct sunlight and fewer living organisms in general. Your task is to design the brain and the phenotypes of a successful organism on this new planet, using some of the neuroscience and machine learning principles you have learned so far. Your designed brain and nervous system will be translated into genetic instructions and inserted into an egg that will be sent to the new planet. Be as specific as you can so your egg doesn’t become a mutant! . If you are stuck, here are some options for getting started . | What do you consider a successful organism? | What kinds of traits would support this success; what kinds of organisms on Earth have these traits? | What parts of the brain support these traits and what kind of machine learning algorithms might also exhibit these abilities? | How might these different traits and abilities need to work together, or connect? | . ",
    "url": "/i2course/megadoc/unit-11/#unit-11-building-a-brain-networks-and-systems",
    
    "relUrl": "/megadoc/unit-11/#unit-11-building-a-brain-networks-and-systems"
  },"41": {
    "doc": "Unit 11",
    "title": "Unit 11",
    "content": " ",
    "url": "/i2course/megadoc/unit-11/",
    
    "relUrl": "/megadoc/unit-11/"
  },"42": {
    "doc": "Unit 12",
    "title": "Unit 12: Human Characteristics of the Brain",
    "content": "Several traits of humans appear to be unique to our species and may be essential in designing an intelligence similar to our own. This chapter is dedicated to the study of these phenomena of our brains. We begin by trying to peel back the layers of humans’ strong social tendencies: . **Task 1: **Complete the following two lectures and synthesis questions. 20. Theory of Mind &amp; Mentalizing . Synthesis Questions: . | Describe the false belief paradigm and what function it serves. | What part of the brain is/are specifically involved in thinking about others' thoughts? Where is it located? What are several close by modules and their functions? | Google the terms TMS, EEG, and DBS. What are several differences and similarities between these cognitive science methods? | . | [Social learning in independent multi-agent reinfor… | Kamal N’dousse | OpenAI Scholars Demo Day 2020](https://www.youtube.com/watch?v=Qy9J5519s68) | . Synthesis Questions: . | What systems might the methods of RL used (DQN, PPO, etc…) lack that make learning theory of mind or social learning more difficult for AI than humans? . | Research several more areas of the brain or human skills implicated in social learning. Does RL or any area of ML research incorporate these concepts? | . | . **Task 2: **Refresh yourself on the use of attention mechanisms in transformers in machine learning with this quick video, then dig into the next lecture and corresponding questions. Attention Mechanism In a Nutshell . For a deeper dive, feel free to take a look at the Language Modeling chapter of this document. Now we will dive into the capabilities of the brain and its own beautiful, endogenous attention mechanisms! . 24. Attention and Awareness . Synthesis Questions: . | Describe the brain's ability to multitask? What are some scenarios where parallel processing is feasible and some where it is not? | Describe covert and overt attention. | Research some machine learning attention mechanisms. Compare and contrast the mechanism and the capabilities of one with what you learned about our biological attention mechanism. | Describe \"priming the visual cortex\" why it might be useful. | \"You have 10x as many connections going down from cortex, down to the LGN ([lateral geniculate nucleus](https://en.wikipedia.org/wiki/Lateral_geniculate_nucleus))… than going forward. One of the things you're doing is setting up selective filters so that only the stuff you want to process makes it to higher stages.\" Compare this fact to deep learning algorithms. | Describe the role of the Fronto-Parietal Attention Network\" | . ",
    "url": "/i2course/megadoc/unit-12/#unit-12-human-characteristics-of-the-brain",
    
    "relUrl": "/megadoc/unit-12/#unit-12-human-characteristics-of-the-brain"
  },"43": {
    "doc": "Unit 12",
    "title": "Unit 12",
    "content": " ",
    "url": "/i2course/megadoc/unit-12/",
    
    "relUrl": "/megadoc/unit-12/"
  },"44": {
    "doc": "Unit A",
    "title": "Unit A: Gradient Descent Deep Dive",
    "content": "A deeper dive into Gradient Descent where you will be implementing backpropagation on your own! This is an involved unit that, while technically not required, will push your understanding of neural networks to the max. Task 1: Watch the following video and implement micrograd as specified: . The spelled-out intro to neural networks and backpropagation: building micrograd . The templates we made for you can be found here. Additionally, please implement the ReLU nonlinearity for the Value class. | (Note: if you’re having a hard time with this, take a look at this code.) | . Implement and train a small neural network using micrograd. The training, validation, and test data will be included in the starter code. - Try to find the best network you can! . You might want to change the learning rate, size of the network, or Note your training, validation, and test loss for your best network. Synthesis Questions: . | In what direction do gradients flow (with regards to loss)? | How do gradients flow through addition? How do they flow through the ReLU function? | What was your best loss for the test dataset? | Was there something that stood out to you? Something that confused you? | What's one resource that was helpful (suggested or found on your own)? | . ",
    "url": "/i2course/megadoc/unit-A/#unit-a-gradient-descent-deep-dive",
    
    "relUrl": "/megadoc/unit-A/#unit-a-gradient-descent-deep-dive"
  },"45": {
    "doc": "Unit A",
    "title": "Unit A",
    "content": " ",
    "url": "/i2course/megadoc/unit-A/",
    
    "relUrl": "/megadoc/unit-A/"
  },"46": {
    "doc": "Unit B",
    "title": "Unit B: Neuroscience of Creativity",
    "content": "Are you curious about the mysterious force of invention, art, mathematics, and more that seems to allow the mind to expand beyond the information it was given to create beautiful new ideas? This is creativity, and with neuroscience, we can begin to dissect it such that we can begin to recreate it. Task 1: Listen or watch the following podcast (also available on Spotify if you prefer) and answer the Synthesis questions! . The Science of Creativity &amp; How to Enhance Creative Innovation - Huberman Lab . Synthesis Questions: . | TODO: add questions | . ",
    "url": "/i2course/megadoc/unit-B/#unit-b-neuroscience-of-creativity",
    
    "relUrl": "/megadoc/unit-B/#unit-b-neuroscience-of-creativity"
  },"47": {
    "doc": "Unit B",
    "title": "Unit B",
    "content": " ",
    "url": "/i2course/megadoc/unit-B/",
    
    "relUrl": "/megadoc/unit-B/"
  },"48": {
    "doc": "Wiki",
    "title": "Wiki",
    "content": "# Wiki **TODO:** do we need this page? ## Content Overview: - ML - Linear Regression - SVM - PCA - K-Means Clustering - DL - Neural network \"anatomy\" - Backpropagation calculus - Neuroanatomy - Neurons - Action potentials - Major brain modules - The Visual System - Primary visual cortex - Computer Vision - Convolutions - Convolutional neural networks - Reinforcement Learning - Key RL vocabulary - Q, V, Bellman Equations - Deep Q Learning - Epsilon greedy explore-exploit - Movement - The cerebellum - Language Modeling - Word Embeddings - Recurrent neural networks - Backpropagation through time - Truncated backpropagation through time - Transformers - Huggingface - AI Fairness/Theory - TBD - Pain in RL/Neuroscience/Cognitive Science - Biological pain in the context of RL - MaxPain Algorithm - Networks and Systems of the Brain - Nature vs. Nurture - Brain Networks And also: - Pytorch - Jupyter notebooks - LaTeX - Self-learning skills ",
    "url": "/i2course/wiki/",
    
    "relUrl": "/wiki/"
  }
}
