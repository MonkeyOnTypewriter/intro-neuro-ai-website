<!DOCTYPE html><html lang="en-US"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=Edge"><link rel="stylesheet" href="/introcourse/assets/css/just-the-docs-default.css"> <script src="/introcourse/assets/js/vendor/lunr.min.js"></script> <script src="/introcourse/assets/js/just-the-docs.js"></script><meta name="viewport" content="width=device-width, initial-scale=1"><title>Unit 08 | I2 Intro Neuro/AI</title><meta name="generator" content="Jekyll v3.9.3" /><meta property="og:title" content="Unit 08" /><meta name="author" content="Interactive Intelligence" /><meta property="og:locale" content="en_US" /><meta name="description" content="Welcome to Interactive Intelligence’s Introduction to Neuro/AI crash course!" /><meta property="og:description" content="Welcome to Interactive Intelligence’s Introduction to Neuro/AI crash course!" /><link rel="canonical" href="http://localhost:4000/introcourse/megadoc/unit-08/" /><meta property="og:url" content="http://localhost:4000/introcourse/megadoc/unit-08/" /><meta property="og:site_name" content="I2 Intro Neuro/AI" /><meta property="og:type" content="website" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Unit 08" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Interactive Intelligence"},"description":"Welcome to Interactive Intelligence’s Introduction to Neuro/AI crash course!","headline":"Unit 08","url":"http://localhost:4000/introcourse/megadoc/unit-08/"}</script><body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"><title>Link</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"><title>Menu</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"><title>Expand</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"><polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"><title id="svg-external-link-title">(external link)</title><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"><title>Document</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"><path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"><title>Search</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <symbol id="svg-copy" viewBox="0 0 16 16"><title>Copy</title><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"><path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/><path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"><title>Copied</title><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"><path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/><path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg><div class="side-bar"><div class="site-header"> <a href="/introcourse/" class="site-title lh-tight"> I2 Intro Neuro/AI </a> <a href="#" id="menu-button" class="site-button"> <svg viewBox="0 0 24 24" class="icon"><use xlink:href="#svg-menu"></use></svg> </a></div><nav aria-label="Main" id="site-nav" class="site-nav"><ul class="nav-list"><li class="nav-list-item"><a href="/introcourse/announcements/" class="nav-list-link">Announcements</a><li class="nav-list-item active"><a href="#" class="nav-list-expander" aria-label="toggle links in Megadoc category"> <svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg> </a><a href="/introcourse/megadoc/" class="nav-list-link">Megadoc</a><ul class="nav-list"><li class="nav-list-item "><a href="/introcourse/megadoc/unit-01/" class="nav-list-link">Unit 01</a><li class="nav-list-item "><a href="/introcourse/megadoc/unit-02/" class="nav-list-link">Unit 02</a><li class="nav-list-item "><a href="/introcourse/megadoc/unit-03/" class="nav-list-link">Unit 03</a><li class="nav-list-item "><a href="/introcourse/megadoc/unit-04/" class="nav-list-link">Unit 04</a><li class="nav-list-item "><a href="/introcourse/megadoc/unit-05/" class="nav-list-link">Unit 05</a><li class="nav-list-item "><a href="/introcourse/megadoc/unit-06/" class="nav-list-link">Unit 06</a><li class="nav-list-item "><a href="/introcourse/megadoc/unit-07/" class="nav-list-link">Unit 07</a><li class="nav-list-item active"><a href="/introcourse/megadoc/unit-08/" class="nav-list-link active">Unit 08</a><li class="nav-list-item "><a href="/introcourse/megadoc/unit-09/" class="nav-list-link">Unit 09</a><li class="nav-list-item "><a href="/introcourse/megadoc/unit-10/" class="nav-list-link">Unit 10</a><li class="nav-list-item "><a href="/introcourse/megadoc/unit-11/" class="nav-list-link">Unit 11</a><li class="nav-list-item "><a href="/introcourse/megadoc/unit-12/" class="nav-list-link">Unit 12</a><li class="nav-list-item "><a href="/introcourse/megadoc/unit-A/" class="nav-list-link">Unit A</a><li class="nav-list-item "><a href="/introcourse/megadoc/unit-B/" class="nav-list-link">Unit B</a></ul><li class="nav-list-item"><a href="/introcourse/schedule/" class="nav-list-link">Schedule</a><li class="nav-list-item"><a href="/introcourse/staff/" class="nav-list-link">Staff</a><li class="nav-list-item"><a href="/introcourse/wiki/" class="nav-list-link">Wiki</a></ul></nav><footer class="site-footer"> This site uses <a href="https://github.com/just-the-docs/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll.</footer></div><div class="main" id="top"><div id="main-header" class="main-header"><div class="search"><div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search I2 Intro Neuro/AI" aria-label="Search I2 Intro Neuro/AI" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label></div><div id="search-results" class="search-results"></div></div><nav aria-label="Auxiliary" class="aux-nav"><ul class="aux-nav-list"><li class="aux-nav-list-item"> <a href="https://interactive-intelligence.github.io" class="site-button" > Interactive Intelligence </a><li class="aux-nav-list-item"> <a href="https://github.com/interactive-intelligence/intro-neuro-ai" class="site-button" > Course GitHub </a></ul></nav></div><div id="main-content-wrap" class="main-content-wrap"><nav aria-label="Breadcrumb" class="breadcrumb-nav"><ol class="breadcrumb-nav-list"><li class="breadcrumb-nav-list-item"><a href="/introcourse/megadoc/">Megadoc</a><li class="breadcrumb-nav-list-item"><span>Unit 08</span></ol></nav><div id="main-content" class="main-content" role="main"><h1 id="unit-8-language-modeling"> <a href="#unit-8-language-modeling" class="anchor-heading" aria-labelledby="unit-8-language-modeling"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Unit 8: Language Modeling</h1><p>Hello and welcome to the <em>Language Modeling (LM)</em> section of the I2 megadoc! Language modeling is an incredibly pertinent field of deep learning that is focused on using statistical modeling to generate sequences of language tokens (I made this term very general on purpose). This can mean Q/A prompts/output like ChatGPT, Seq2Seq models for machine translation, or sentiment analysis on text messages. Language Modeling, like Reinforcement Learning, is an incredibly large subfield that we simply cannot do justice in 1 unit. We will, however, try to introduce you to some basic language Modeling Deep Learning architectures that get more and more complex. We will also introduce you to <em>HuggingFace</em>, a service that allows you to learn about, train, load, finetune, and save DL models.</p><p>First, we will explore the concept of word embeddings. Understanding this is crucial to clearing up some FAQ about Language Modeling. So far most of the data you have worked with has been easily “castable” into n-dimensional vectors. However, when we are working with language, how can we convert a word like “King” into a vector? Performing computation directly on words is an incredibly difficult task. However, if we can “encode” the meaning of the word “King” into a vector consisting of numbers and have some way of converting between the two then the problem becomes easier.</p><p><strong>Task:</strong> Read this “<a href="https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/">Word Embeddings</a>” article and answer the following synthesis questions:</p><p>You may skip the “Recursive Neural Networks” subsection of the article if you wish</p><h3 id="synthesis-questions"> <a href="#synthesis-questions" class="anchor-heading" aria-labelledby="synthesis-questions"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <code class="language-plaintext highlighter-rouge">Synthesis Questions:</code></h3><ul><li><code class="language-plaintext highlighter-rouge">What is a word embedding? How long are they usually?</code><li><code class="language-plaintext highlighter-rouge">How does training a network to recognize the validity of 5-grams result in a Word-to-Vector "map"?</code><ul><li><code class="language-plaintext highlighter-rouge">Can you think of another training method to achieve the same side-effect?</code></ul><li><code class="language-plaintext highlighter-rouge">Pretend you are a word embedder. Give examples (2-3 for each) of words in the same family as:</code><ul><li><code class="language-plaintext highlighter-rouge">King</code><li><code class="language-plaintext highlighter-rouge">Button</code><li><code class="language-plaintext highlighter-rouge">Pain</code><li><code class="language-plaintext highlighter-rouge">Water Bottle</code></ul><li><code class="language-plaintext highlighter-rouge">What is the explanation given for the emergence of the "male-female difference vector"?</code><li><code class="language-plaintext highlighter-rouge">What is pre training/transfer learning/multi-task learning?</code></ul><p>Now we go into Recurrent Neural Networks (RNN’s) and the concept of Backpropagation Through Time (BPTT) and its drawbacks. This builds off of Unit 2 (Deep learning) so feel free to revisit those if you are having trouble!</p><p><strong>Task:</strong> Read this “<a href="https://blog.paperspace.com/recurrent-neural-networks-part-1-2/">How RNN’s Work</a>” article and answer the following synthesis questions:</p><p>You may skip the “Word Embedding” and “Backward Pass” subsections of the article if you wish. There is a better resource below that explains the concept of a Backwards Pass in RNNs (BPTT)</p><p><strong>Helpful clarifications for after you read:</strong></p><ul><li>Note that what is “inside” the RNN unit (the circles on the computational graph) is modifiable, and I have attached here a simple look into the inside:</ul><p><img src="../assets/image7.png" alt="alt_text" /></p><ul><li>I also have attached an optional reading about <a href="https://d2l.ai/chapter_recurrent-modern/gru.html">Gated Recurrent Units</a> (GRU’s) here. “GRUs have the ability to keep memory/state from previous activations rather than replacing the entire activation like a vanilla RNN, allowing them to remember features for a long time and allowing backpropagation to happen through multiple bounded nonlinearities, which reduces the likelihood of the vanishing gradient” (<a href="https://medium.com/paper-club/grus-vs-lstms-e9d8e2484848">GRUs vs LSTMs</a>).</ul><h3 id="synthesis-questions-1"> <a href="#synthesis-questions-1" class="anchor-heading" aria-labelledby="synthesis-questions-1"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <code class="language-plaintext highlighter-rouge">Synthesis Questions:</code></h3><ul><li><code class="language-plaintext highlighter-rouge">What does a language model, in general, try to predict?</code><ul><li><code class="language-plaintext highlighter-rouge">Hint: Predicting X given Y. What are X and Y?</code></ul><li><code class="language-plaintext highlighter-rouge">What happens to the memory vector as we move through time?</code><li><code class="language-plaintext highlighter-rouge">Describe how a RNN would deal with the sentence "How are you?" in terms of its unrolled computational graph</code><ul><li><code class="language-plaintext highlighter-rouge">Basically, what happens to these words and the hidden states generated from these words?</code><li><code class="language-plaintext highlighter-rouge">How long would the unrolled computational graph be in terms of RNN nodes (circles)?</code></ul><li><code class="language-plaintext highlighter-rouge">What is the memory vector initialized to?</code><li><code class="language-plaintext highlighter-rouge">What is &amp;lt;\s&gt;? What does it signify according to the article?</code></ul><p><strong>Task:</strong> Read this “<a href="https://www.geeksforgeeks.org/ml-back-propagation-through-time/">Backpropagation Through Time</a>” (BPTT) article, a small extension: “<a href="http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf">Truncated BPTT</a>” <strong>(read only section 2.8.6)</strong> and answer the following synthesis questions:</p><h3 id="synthesis-questions-2"> <a href="#synthesis-questions-2" class="anchor-heading" aria-labelledby="synthesis-questions-2"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <code class="language-plaintext highlighter-rouge">Synthesis Questions:</code></h3><p><strong>TODO:</strong> what are the sub tags?</p><ul><li><code class="language-plaintext highlighter-rouge">What are W&lt;sub&gt;x&lt;/sub&gt;, W&lt;sub&gt;y&lt;/sub&gt;, and W&lt;sub&gt;s&lt;/sub&gt;?</code><li><code class="language-plaintext highlighter-rouge">Why does BPTT not work with a large number of timesteps?</code><ul><li><code class="language-plaintext highlighter-rouge">What is this problem called?</code></ul><li><code class="language-plaintext highlighter-rouge">How does Truncated BPTT solve this problem?</code></ul><p>Finally we get into the Transformer architecture, which as of 2023 seems to have a grip on the DL market as the most generally powerful type of neural network used in all sorts of LM tasks. <strong>This subsection is especially difficult. Please read the articles slowly and carefully. Be sure to ask for help if you have any questions or trouble understanding a statement!</strong></p><p><strong>Task:</strong> Read the following “Intuitive Explanation of GPT Models” <a href="https://cswartout.com/2022/11/25/intutive-explanation-of-gpt.html">Part 1</a> and <a href="https://cswartout.com/2022/12/25/intuitive-explanation-of-gpt-part-2.html">Part 2</a> (and hopefully part 3 soon!) (Made by University of Washington Interactive Intelligence member Carter!) and answer the following synthesis questions:</p><p><strong>In case this article is not enough, or you find yourself struggling to fully understand transformers, take a look at this list of other links that can reinforce your knowledge:</strong></p><ul><li><a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a> Another good explanation of Transformers.<li><a href="https://jaykmody.com/blog/gpt-from-scratch/">GPT in 60 lines of NumPy</a> - explains GPT in a concise way using code!<li><a href="http://nlp.seas.harvard.edu/annotated-transformer/">The Annotated Transformer</a> - the <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a> paper annotated with PyTorch.<li><a href="https://github.com/karpathy/nanoGPT">NanoGPT</a>, simple example code for a GPT model.</ul><h3 id="synthesis-questions-3"> <a href="#synthesis-questions-3" class="anchor-heading" aria-labelledby="synthesis-questions-3"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <code class="language-plaintext highlighter-rouge">Synthesis Questions:</code></h3><ul><li><code class="language-plaintext highlighter-rouge">Why does not being able to capture long-term dependencies result in nonsensical generated paragraphs (like clicking autocomplete)?</code><li><code class="language-plaintext highlighter-rouge">What do the probabilities that GPT outputs represent, and what is greedy decoding?</code><li><code class="language-plaintext highlighter-rouge">What is a token?</code><li><code class="language-plaintext highlighter-rouge">What are the big blocks that make up the GPT architecture?</code><li><code class="language-plaintext highlighter-rouge">What are the two main blocks inside a transformer-decoder block?</code><li><code class="language-plaintext highlighter-rouge">Describe masked self-attention in your own words (not including the vector math).</code><ul><li><code class="language-plaintext highlighter-rouge">How would this help stop generated sequences from being mostly nonsensical?</code></ul><li><code class="language-plaintext highlighter-rouge">How are Query, Key, and Value vectors generated from each word embedding?</code><li><code class="language-plaintext highlighter-rouge">How is the score for each word calculated using one word's query vector and all the other words' key vectors?</code><li><code class="language-plaintext highlighter-rouge">How is the score for each word and its value vector used to create the vector for a single transformed word? (In the case of the article, the word is "He").</code><li><code class="language-plaintext highlighter-rouge">Briefly describe multi-headed attention</code></ul><p>Awesome job answering those synthesis questions for Transformers! Now we move onto the project (which won’t be nearly as difficult)</p><p><strong>Project Spec:</strong></p><p>The project for this “<em>Language Modeling</em>” section will be following the tutorial/Jupyter Notebook below. Please ask questions in the discord as you work through this project. Be sure to discuss with others in your group!</p><p>A few helpful tips:</p><ul><li>Use GitHub, it’s really just better<li>Use <a href="https://www.anaconda.com/">Anaconda</a> with <a href="https://www.python.org/downloads/">Python3</a> in <a href="https://code.visualstudio.com/">VSCode</a>.<ul><li>If you use Anaconda, create a separate environment so you can mess with libraries and imports all day without screwing up your base environment.</ul><li>Leave comments to cement your understanding. Link syntax to ideas.</ul><p><strong>Clone the Git repo onto your local device if you have not already.</strong></p><p><strong><span style="text-decoration:underline;">There are 2 parts (.ipynb files) to this unit. Finish both.</span></strong></p><p>In your local copy of the GitHub repo, navigate to the unit-8 folder, and work on <strong>hf_tutorial.ipynb</strong>. Instructions are in the Jupyter notebook. If you need help setting up your python environment, ask the TA’s!</p><p><strong>GH Link:</strong> <a href="https://github.com/interactive-intelligence/intro-neuro-ai/blob/main/unit-08/hf_tutorial.ipynb">Unit 8 Notebook Part 1</a>** (1 hr)**</p><p>Now navigate to the application portion of this project, where you are given a dataset and asked to train an LLM of your choice to emulate Shakespeare! Be sure to reference your Unit 8 Notebook Part 1 to figure out how to do this. The starter code is in <strong>lm_starter_code.ipynb</strong></p><p><strong>GH Link:</strong> <a href="https://github.com/interactive-intelligence/intro-neuro-ai/blob/main/unit-08/lm_starter_code.ipynb">Unit 8 Notebook Part 2</a> <strong>(1 hr)</strong></p><p>When you are finished with your code, independently verify that it works and have fun with it! If you add any additional functionality be sure to talk about it with others and give them ideas.</p><p>Congratulations! You now understand the basics of Language Modeling!</p></div></div><div class="search-overlay"></div></div>
