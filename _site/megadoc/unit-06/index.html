<!DOCTYPE html><html lang="en-US"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=Edge"><link rel="stylesheet" href="/i2course/assets/css/just-the-docs-default.css"> <script src="/i2course/assets/js/vendor/lunr.min.js"></script> <script src="/i2course/assets/js/just-the-docs.js"></script><meta name="viewport" content="width=device-width, initial-scale=1"><title>Unit 06 | I2 Intro Neuro/AI</title><meta name="generator" content="Jekyll v3.9.3" /><meta property="og:title" content="Unit 06" /><meta name="author" content="Interactive Intelligence" /><meta property="og:locale" content="en_US" /><meta name="description" content="Welcome to Interactive Intelligence’s Introduction to Neuro/AI crash course!" /><meta property="og:description" content="Welcome to Interactive Intelligence’s Introduction to Neuro/AI crash course!" /><link rel="canonical" href="http://localhost:3001/i2course/megadoc/unit-06/" /><meta property="og:url" content="http://localhost:3001/i2course/megadoc/unit-06/" /><meta property="og:site_name" content="I2 Intro Neuro/AI" /><meta property="og:type" content="website" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Unit 06" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Interactive Intelligence"},"description":"Welcome to Interactive Intelligence’s Introduction to Neuro/AI crash course!","headline":"Unit 06","url":"http://localhost:3001/i2course/megadoc/unit-06/"}</script><body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"><title>Link</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"><title>Menu</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"><title>Expand</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"><polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"><title id="svg-external-link-title">(external link)</title><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"><title>Document</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"><path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"><title>Search</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <symbol id="svg-copy" viewBox="0 0 16 16"><title>Copy</title><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"><path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/><path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"><title>Copied</title><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"><path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/><path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg><div class="side-bar"><div class="site-header"> <a href="/i2course/" class="site-title lh-tight"> I2 Intro Neuro/AI </a> <a href="#" id="menu-button" class="site-button"> <svg viewBox="0 0 24 24" class="icon"><use xlink:href="#svg-menu"></use></svg> </a></div><nav aria-label="Main" id="site-nav" class="site-nav"><ul class="nav-list"><li class="nav-list-item"><a href="/i2course/announcements/" class="nav-list-link">Announcements</a><li class="nav-list-item active"><a href="#" class="nav-list-expander" aria-label="toggle links in Megadoc category"> <svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg> </a><a href="/i2course/megadoc/" class="nav-list-link">Megadoc</a><ul class="nav-list"><li class="nav-list-item "><a href="/i2course/megadoc/unit-01/" class="nav-list-link">Unit 01</a><li class="nav-list-item "><a href="/i2course/megadoc/unit-02/" class="nav-list-link">Unit 02</a><li class="nav-list-item "><a href="/i2course/megadoc/unit-03/" class="nav-list-link">Unit 03</a><li class="nav-list-item "><a href="/i2course/megadoc/unit-04/" class="nav-list-link">Unit 04</a><li class="nav-list-item "><a href="/i2course/megadoc/unit-05/" class="nav-list-link">Unit 05</a><li class="nav-list-item active"><a href="/i2course/megadoc/unit-06/" class="nav-list-link active">Unit 06</a><li class="nav-list-item "><a href="/i2course/megadoc/unit-07/" class="nav-list-link">Unit 07</a><li class="nav-list-item "><a href="/i2course/megadoc/unit-08/" class="nav-list-link">Unit 08</a><li class="nav-list-item "><a href="/i2course/megadoc/unit-09/" class="nav-list-link">Unit 09</a><li class="nav-list-item "><a href="/i2course/megadoc/unit-10/" class="nav-list-link">Unit 10</a><li class="nav-list-item "><a href="/i2course/megadoc/unit-11/" class="nav-list-link">Unit 11</a><li class="nav-list-item "><a href="/i2course/megadoc/unit-12/" class="nav-list-link">Unit 12</a><li class="nav-list-item "><a href="/i2course/megadoc/unit-A/" class="nav-list-link">Unit A</a><li class="nav-list-item "><a href="/i2course/megadoc/unit-B/" class="nav-list-link">Unit B</a></ul><li class="nav-list-item"><a href="/i2course/schedule/" class="nav-list-link">Schedule</a><li class="nav-list-item"><a href="/i2course/staff/" class="nav-list-link">Staff</a><li class="nav-list-item"><a href="/i2course/wiki/" class="nav-list-link">Wiki</a></ul></nav><footer class="site-footer"> This site uses <a href="https://github.com/just-the-docs/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll.</footer></div><div class="main" id="top"><div id="main-header" class="main-header"><div class="search"><div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search I2 Intro Neuro/AI" aria-label="Search I2 Intro Neuro/AI" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label></div><div id="search-results" class="search-results"></div></div><nav aria-label="Auxiliary" class="aux-nav"><ul class="aux-nav-list"><li class="aux-nav-list-item"> <a href="https://interactive-intelligence.github.io" class="site-button" > Interactive Intelligence </a><li class="aux-nav-list-item"> <a href="https://github.com/interactive-intelligence/intro-neuro-ai" class="site-button" > Course GitHub </a></ul></nav></div><div id="main-content-wrap" class="main-content-wrap"><nav aria-label="Breadcrumb" class="breadcrumb-nav"><ol class="breadcrumb-nav-list"><li class="breadcrumb-nav-list-item"><a href="/i2course/megadoc/">Megadoc</a><li class="breadcrumb-nav-list-item"><span>Unit 06</span></ol></nav><div id="main-content" class="main-content" role="main"><h1 id="unit-6-reinforcement-learning"> <a href="#unit-6-reinforcement-learning" class="anchor-heading" aria-labelledby="unit-6-reinforcement-learning"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Unit 6: Reinforcement Learning</h1><p>Hello and welcome to the <em>Reinforcement Learning (RL)</em> section of the I2 megadoc! Reinforcement learning is an incredibly powerful subfield of Machine Learning that can be used to deploy intelligent autonomous agents in both real and simulated systems. Reinforcement learning in itself is not inherently tied to AI. It is its own field that has strong ties to probability and statistics theory.</p><p>For the purposes of simplicity, we will only cover the <em>very</em> basics and terminology of Deep RL (RL combined with deep learning). Just as a heads up, RL is probably one of the most “deep” and currently relevant subfields of ML, followed closely by language modeling (this may also be the Dunning-Kruger effect, RL is what I have the most expertise on). Folks at OpenAI have compiled a plethora of resources for Deep RL and Deep RL research. We will show you the most relevant articles they have published but they have a wealth of knowledge to learn! Feel free to poke around on the <a href="https://spinningup.openai.com/en/latest/index.html">Spinning Up</a> page in your own time.</p><p><strong>Task:</strong> Read the <a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#">Key Concepts in RL</a> section of the Spinning Up Intro to RL page. <strong>This is a short, but <em>dense</em> article. (45 min)</strong></p><p>You may skip the section on Diagonal Gaussian Policies.</p><h3 id="synthesis-questions"> <a href="#synthesis-questions" class="anchor-heading" aria-labelledby="synthesis-questions"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <code class="language-plaintext highlighter-rouge">Synthesis Questions:</code></h3><ul><li><code class="language-plaintext highlighter-rouge">In what situations do we use reinforcement learning? What kinds of problems does it solve?</code><ul><li><code class="language-plaintext highlighter-rouge">Give an example of RL used to play a game. Did it outperform humans? Does this scare you?</code></ul><li><code>What do the variables <strong>s</strong>, <strong>a</strong>, <strong>τ</strong>, represent in RL?</code><ul><li><code>What do <strong>s'</strong>, <strong>a' </strong> represent?</code></ul><li><code>What is an action space?</code><li><code>What is an episode?</code><li><code>How is a neural network used to generate a probability distribution over actions given a state? </code><ul><li><code>(i.e How would you construct a network if the dimensionality of the state was n<sub>1</sub> and the action space had a cardinality of m<sub>1</sub>)</code><li><code class="language-plaintext highlighter-rouge">How is the log probability calculated from the logits of the NN output?</code><li><code class="language-plaintext highlighter-rouge">Note: This is called an actor NN!</code></ul><li><code class="language-plaintext highlighter-rouge">Describe infinite-horizon discounted return and what the discount factor is</code><li><code class="language-plaintext highlighter-rouge">Describe what a stochastic policy is (you may have to look up what stochasticity is), and what it means for a policy to be parameterized by θ</code><li><code class="language-plaintext highlighter-rouge">What is a good policy looking to maximize?</code><ul><li><code class="language-plaintext highlighter-rouge">Think: If you set up your rewards in your environment randomly, would an RL model learn anything? No. This ties into the greater idea of reward-shaping, which is the concept of how to place rewards in an environment to encourage "good" behavior by an RL agent.</code></ul><li><code class="language-plaintext highlighter-rouge">What is the difference between the value function and the Q-function?</code><ul><li><code class="language-plaintext highlighter-rouge">Describe the connection between the two</code></ul><li><code>Based on the idea behind Bellman Equations<strong> ("The value of your starting point is the reward you expect to get from being there, plus the value of wherever you land next.") </strong>Explain how the following two equations satisfy the idea behind Bellman Equations: </code></ul><p><img src="../assets/image3.png" alt="alt_text" title="image_tooltip" /></p><p><code class="language-plaintext highlighter-rouge">This may help:</code></p><p><img src="../assets/image4.png" alt="alt_text" title="image_tooltip" /></p><ul><li><code>What is the advantage function?</code></ul><p>This was a lot of content. Good job on getting through it all! Reinforcement learning is quite a math heavy subfield. Get used to a whole lot of probability and integration. Spelling out exactly what the equations are doing (by reading them out loud) can help quite a bit. Next we will be diving into some in-practice algorithms on a high level.</p><p>So, in general, these equations that you have learned about are ideals, not what is done in practice. What generally happens with deep learning RL algorithms is that these functions are **approximated **by a neural network rather than calculated. This is important to keep in mind as you go on, because a lot of these equations have some recursiveness to them and often have such a huge search space that finding the perfect parameters for V<sup>*</sup> or Q<sup>*</sup> is near impossible in complex environments.</p><p><strong>Task:</strong> Read the “<a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html">Kinds of RL Algorithms</a>” section of the Spinning Up Intro to RL page.** (15 min)**</p><p>You may skip the section on What to Learn in Model-Based RL</p><h3 id="synthesis-questions-1"> <a href="#synthesis-questions-1" class="anchor-heading" aria-labelledby="synthesis-questions-1"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <code class="language-plaintext highlighter-rouge">Synthesis Questions:</code></h3><ul><li><code class="language-plaintext highlighter-rouge">What is the difference between model-free and model-based RL?</code><li><code class="language-plaintext highlighter-rouge">What is the difference between policy approximators and Q-learning?</code><li><code>Think back to what the J([π](https://www.pisymbol.net/#:~:text=Alt%2B960%20Press%20and%20hold,enter%20960%20on%20numeric%20keypad.)) function was. How would performing gradient ascent (finding where this function is locally highest) on this function help the agent perform well in its environment?</code><li><code>How is a policy derived (not literally a derivation) from the Q-function? Hint: argmax</code></ul><p>Great job on completing the tasks!</p><p>The last thing I will talk about before the project is the concept of <strong>epsilon-greediness</strong> for Q-learning. In essence, argmaxing the Q-function can lead to being stuck in a “pocket” where the agent gets stuck taking the same few trajectories due to the deterministic selection of the action to take. Since Q-learning is usually model-free, there is no way to “see” what lies beyond what is explored. Since the model will not take the actions to explore these trajectories, possibly more efficient solutions cannot be stumbled on by chance. This is where epsilon greediness comes in. With probability <a href="http://www.unicode-symbol.com/u/03B5.html">ε</a>, the policy will take a random action rather than the one that maximizes the Q-function. <a href="http://www.unicode-symbol.com/u/03B5.html">Ε</a> starts off large and diminishes over time, as the model is expected to have learned better by then and there is less need to explore rather than exploit the learned model for high returns. You can see how epsilon changes through episodes in the graph below.</p><p><img src="../assets/image9.png" alt="alt_text" /></p><p>You can read more about epsilon-greedy algorithms in <a href="https://www.baeldung.com/cs/epsilon-greedy-q-learning#:~:text=The%20epsilon%2Dgreedy%20approach%20selects,what%20we%20have%20already%20learned.">this</a> article (Section 5.2). It works quite well and often gets a model “unstuck” from optimizing poorly through not exploring.</p><p><strong>Project Spec:</strong></p><p>The project for this “<em>Reinforcement Learning</em>” section will be following the tutorial/Jupyter Notebook below. Please ask questions in the discord as you work through this project. Be sure to discuss with others in your group!</p><p>A few helpful tips:</p><ul><li>Use GitHub, it’s really just better<li>Use <a href="https://www.anaconda.com/">Anaconda</a> with <a href="https://www.python.org/downloads/">Python3</a> in <a href="https://code.visualstudio.com/">VSCode</a>.<ul><li>If you use Anaconda, create a separate environment so you can mess with libraries and imports all day without screwing up your base environment.</ul><li>Type most of the code out yourself instead of just copying from the tutorial.<li>Leave comments to cement your understanding. Link syntax to ideas.</ul><p>Then, in your local copy of the GitHub repo, navigate to the unit-6 folder, and work on <strong>rl_net.ipynb</strong>. Instructions are in the Jupyter notebook. If you need help setting up your python environment, ask the TA’s!</p><p><strong>GH Link:</strong> <a href="https://github.com/interactive-intelligence/intro-neuro-ai/blob/main/unit-6/rl_net.ipynb">Unit 6 Notebook</a> <strong>(1.5 hr)</strong></p><ul><li>In case there is an error with the notebook not showing up on GitHub, paste the link into <a href="https://kokes.github.io/nbviewer.js/viewer.html">this</a> website to view it.</ul><p>When you are finished with your code, independently verify that it works and have fun with it! If you add any additional functionality be sure to talk about it with others and give them ideas.</p><p>Congratulations! You now understand the (<em>incredibly basic)</em> basics of Deep RL!</p></div></div><div class="search-overlay"></div></div>
