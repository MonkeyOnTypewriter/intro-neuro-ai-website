<!DOCTYPE html><html lang="en-US"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=Edge"><link rel="stylesheet" href="/i2course/assets/css/just-the-docs-default.css"> <script src="/i2course/assets/js/vendor/lunr.min.js"></script> <script src="/i2course/assets/js/just-the-docs.js"></script><meta name="viewport" content="width=device-width, initial-scale=1"><title>Unit 01 | I2 Intro Neuro/AI</title><meta name="generator" content="Jekyll v3.9.3" /><meta property="og:title" content="Unit 01" /><meta name="author" content="Interactive Intelligence" /><meta property="og:locale" content="en_US" /><meta name="description" content="Welcome to Interactive Intelligence’s Introduction to Neuro/AI crash course!" /><meta property="og:description" content="Welcome to Interactive Intelligence’s Introduction to Neuro/AI crash course!" /><link rel="canonical" href="http://localhost:3001/i2course/megadoc/unit-01/" /><meta property="og:url" content="http://localhost:3001/i2course/megadoc/unit-01/" /><meta property="og:site_name" content="I2 Intro Neuro/AI" /><meta property="og:type" content="website" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Unit 01" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Interactive Intelligence"},"description":"Welcome to Interactive Intelligence’s Introduction to Neuro/AI crash course!","headline":"Unit 01","url":"http://localhost:3001/i2course/megadoc/unit-01/"}</script><body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"><title>Link</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"><title>Menu</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"><title>Expand</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"><polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"><title id="svg-external-link-title">(external link)</title><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"><title>Document</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"><path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"><title>Search</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <symbol id="svg-copy" viewBox="0 0 16 16"><title>Copy</title><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"><path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/><path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"><title>Copied</title><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"><path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/><path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg><div class="side-bar"><div class="site-header"> <a href="/i2course/" class="site-title lh-tight"> I2 Intro Neuro/AI </a> <a href="#" id="menu-button" class="site-button"> <svg viewBox="0 0 24 24" class="icon"><use xlink:href="#svg-menu"></use></svg> </a></div><nav aria-label="Main" id="site-nav" class="site-nav"><ul class="nav-list"><li class="nav-list-item"><a href="/i2course/announcements/" class="nav-list-link">Announcements</a><li class="nav-list-item active"><a href="#" class="nav-list-expander" aria-label="toggle links in Megadoc category"> <svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg> </a><a href="/i2course/megadoc/" class="nav-list-link">Megadoc</a><ul class="nav-list"><li class="nav-list-item active"><a href="/i2course/megadoc/unit-01/" class="nav-list-link active">Unit 01</a><li class="nav-list-item "><a href="/i2course/megadoc/unit-02/" class="nav-list-link">Unit 02</a><li class="nav-list-item "><a href="/i2course/megadoc/unit-03/" class="nav-list-link">Unit 03</a><li class="nav-list-item "><a href="/i2course/megadoc/unit-04/" class="nav-list-link">Unit 04</a><li class="nav-list-item "><a href="/i2course/megadoc/unit-05/" class="nav-list-link">Unit 05</a><li class="nav-list-item "><a href="/i2course/megadoc/unit-06/" class="nav-list-link">Unit 06</a><li class="nav-list-item "><a href="/i2course/megadoc/unit-07/" class="nav-list-link">Unit 07</a><li class="nav-list-item "><a href="/i2course/megadoc/unit-08/" class="nav-list-link">Unit 08</a><li class="nav-list-item "><a href="/i2course/megadoc/unit-09/" class="nav-list-link">Unit 09</a><li class="nav-list-item "><a href="/i2course/megadoc/unit-10/" class="nav-list-link">Unit 10</a><li class="nav-list-item "><a href="/i2course/megadoc/unit-11/" class="nav-list-link">Unit 11</a><li class="nav-list-item "><a href="/i2course/megadoc/unit-12/" class="nav-list-link">Unit 12</a><li class="nav-list-item "><a href="/i2course/megadoc/unit-A/" class="nav-list-link">Unit A</a><li class="nav-list-item "><a href="/i2course/megadoc/unit-B/" class="nav-list-link">Unit B</a></ul><li class="nav-list-item"><a href="/i2course/schedule/" class="nav-list-link">Schedule</a><li class="nav-list-item"><a href="/i2course/staff/" class="nav-list-link">Staff</a><li class="nav-list-item"><a href="/i2course/wiki/" class="nav-list-link">Wiki</a></ul></nav><footer class="site-footer"> This site uses <a href="https://github.com/just-the-docs/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll.</footer></div><div class="main" id="top"><div id="main-header" class="main-header"><div class="search"><div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search I2 Intro Neuro/AI" aria-label="Search I2 Intro Neuro/AI" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label></div><div id="search-results" class="search-results"></div></div><nav aria-label="Auxiliary" class="aux-nav"><ul class="aux-nav-list"><li class="aux-nav-list-item"> <a href="https://interactive-intelligence.github.io" class="site-button" > Interactive Intelligence </a><li class="aux-nav-list-item"> <a href="https://github.com/interactive-intelligence/intro-neuro-ai" class="site-button" > Course GitHub </a></ul></nav></div><div id="main-content-wrap" class="main-content-wrap"><nav aria-label="Breadcrumb" class="breadcrumb-nav"><ol class="breadcrumb-nav-list"><li class="breadcrumb-nav-list-item"><a href="/i2course/megadoc/">Megadoc</a><li class="breadcrumb-nav-list-item"><span>Unit 01</span></ol></nav><div id="main-content" class="main-content" role="main"><h1 id="unit-1-the-machine-learning-basics"> <a href="#unit-1-the-machine-learning-basics" class="anchor-heading" aria-labelledby="unit-1-the-machine-learning-basics"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Unit 1: The (machine learning) Basics</h1><p>Hello and welcome to the <em>Basics</em> section of the I2 megadoc! The items here are fundamental building blocks for Deep Learning (powerful tools that are more complex in computation, but funnily enough not as technical). A lot of the things here are statistics-heavy so be sure to pay attention! We will start off with something I am sure most of you are familiar with - linear regression.</p><p><strong>Task:</strong> Read <a href="https://towardsdatascience.com/the-basics-linear-regression-2fc9f5124687">this</a> article and answer the synthesis questions below! <strong>(15 min)</strong></p><p>Before you begin! There are a few points in this article that I believe are more confusing than helpful, so feel free to ignore them if they do not make complete sense. Keep this in mind as you read, just take note, and don’t stress about the sections I overview below:</p><ul><li>In the subheader “Assumptions and caveats”, there is a bullet titled <em>Output variable is a linear combination of feature variables — linearity</em>. The author claims that linear regression can be used to find non-linear trends. He is right! The methods described check out and make sense. However, this is not what I would consider “simple linear regression” and can be considered a cool footnote. A computer will be handling most of this for you anyways. It is a good rule of thumb to apply linear regression models to linearly correlated data!<li>In the subheader “Assumptions and caveats”, there is a bullet titled <em>Constant variance — homoscedasticity</em>. There is a part towards the end that begins with the words “Practically speaking, we can account for this in one model without splitting out data into three groups…”. The intuition behind the author’s solution to the problem he posed earlier makes sense, but strays from simple linear regression once again. The intuition behind the problem of heteroscedasticity is valid however, so be sure to take note of that.</ul><h3 id="synthesis-questions"> <a href="#synthesis-questions" class="anchor-heading" aria-labelledby="synthesis-questions"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <code class="language-plaintext highlighter-rouge">Synthesis Questions:</code></h3><ul><li><code class="language-plaintext highlighter-rouge">What is a feature in this context?</code><li><code class="language-plaintext highlighter-rouge">What are the significance of the β terms within the modified y = mx + b equation described in the article?</code><li><code class="language-plaintext highlighter-rouge">What is SSE?</code><ul><li><code class="language-plaintext highlighter-rouge">How is it calculated?</code><li><code class="language-plaintext highlighter-rouge">What can it tell you about the values you chose for β?</code><li><code class="language-plaintext highlighter-rouge">If you modify the β&lt;sub&gt;1 &lt;/sub&gt;term and the SSE goes up, was that a good modification?</code></ul><li><code class="language-plaintext highlighter-rouge">Write out the linear regression formula (involving β) when you wish to estimate the impact of age, height, and weight of someone regarding their marital status.</code><ul><li><code class="language-plaintext highlighter-rouge">Hint: How many β terms will there be? How many features?</code></ul><li><code class="language-plaintext highlighter-rouge">What are the 4 assumptions described that you need to confirm before using linear regression?</code><li><code class="language-plaintext highlighter-rouge">What is homoscedasticity? What is heteroscedasticity?</code></ul><p>Also skim over <a href="https://medium.datadriveninvestor.com/basics-of-linear-regression-9b529aeaa0a5">this</a> article and focus on the equation provided. No questions!</p><p>Something to note about ML in general: high-dimensional stuff is simply not visualizable as-is. If you have more than two features it’s near impossible to visualize the spread of dependent variables on a graph with the features as independent variables (How would you graph in 4d like we do in 2d or 3d?). Just know that equations of best fits for linear regressions define hyperplanes of the dimensions that the variables occupy. What is a hyperplane? Well, for example, if a space is 3-dimensional then its hyperplanes are the 2-dimensional planes, while if the space is 2-dimensional, its hyperplanes are the 1-dimensional lines. Hyperplanes are one dimension less than the space they “draw” through. Think about why it’s necessary to have a 2-d hyperplane for a 3d space of prediction (2 features) and a 1-d hyperplane for a 2d space of prediction (1 feature). While not visualizable, this reasoning applies to higher dimensions! Hyperplanes will become important as we move into dimensionality reduction so read up on them if you have time.</p><p>The next topic to cover will be Support Vector Machines (SVMs).</p><p><strong>Task:</strong> Watch and understand the following videos. We recommend taking notes and being able to answer the synthesis questions provided below. Send your I2 teacher/mentor/overlord the answers to the questions over Discord.</p><p>This first video is intuition only.</p><p><strong>Video 1:</strong> <a href="https://www.youtube.com/watch?v=iEQ0e-WLgkQ">Support Vector Machines: Data Science Concepts</a> <strong>(8 min)</strong></p><h3 id="synthesis-questions-1"> <a href="#synthesis-questions-1" class="anchor-heading" aria-labelledby="synthesis-questions-1"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <code class="language-plaintext highlighter-rouge">Synthesis Questions:</code></h3><ul><li><code class="language-plaintext highlighter-rouge">What are some use cases for an SVM? What does it do?</code><li><code class="language-plaintext highlighter-rouge">What is the margin?</code><li><code class="language-plaintext highlighter-rouge">Why does the margin need to be maximized? What does this allow for?</code><li><code class="language-plaintext highlighter-rouge">What are the support vectors?</code><li><code class="language-plaintext highlighter-rouge">What is the difference between a hard and soft margin SVM?</code></ul><p>This next video is math heavy. If you do not understand a term, look it up! Remember that the coefficients for a plane in the form (<code class="language-plaintext highlighter-rouge">ax + by + cz = 0)</code> can be found by determining a “normal” vector (a vector orthogonal to the plane). The vector will consist of numbers a, b, and c! This intuition carries over to higher dimensions (hyperplanes).</p><p><strong>Video 2:</strong> <a href="https://www.youtube.com/watch?v=bM4_AstaBZo">SVM (The Math): Data Science Concepts</a> <strong>(10 min)</strong></p><h3 id="synthesis-questions-2"> <a href="#synthesis-questions-2" class="anchor-heading" aria-labelledby="synthesis-questions-2"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <code class="language-plaintext highlighter-rouge">Synthesis Questions:</code></h3><ul><li><code class="language-plaintext highlighter-rouge">wx - b = -1 defines a __________ </code><ul><li><code class="language-plaintext highlighter-rouge">Hint: it's like a plane</code></ul><li><code class="language-plaintext highlighter-rouge">What are the "w" variables in the equation?</code><li><code class="language-plaintext highlighter-rouge">What is the equation for the decision boundary?</code><li><code class="language-plaintext highlighter-rouge">What is the size of the margin in terms of the vector w?</code><li><code class="language-plaintext highlighter-rouge">What needs to be minimized, and what are the constraints for finding the optimal hyperplane decision boundary? </code><ul><li><code class="language-plaintext highlighter-rouge">Hint: Near the end of the video</code></ul></ul><p>So far you know about Linear Regression and SVMs! Take a moment to make sure you get the general idea of these concepts. We will now be moving into the idea of dimensionality reduction.</p><p>As you may have noticed. A lot of what is done in the ML world is done in way more than three dimensions. Try as you might, you simply cannot accurately envision much above three dimensions concretely (Try and make a 4-d graph). We represent high-dimensional data in vectors, which is a nice numerical representation. But what if we want to see 100-dimensional data on a graph? This is where dimensionality reduction comes into play. We will cover a very basic dimensionality reduction algorithm. There are plenty more that have specific use cases so please** spend at least 10 minutes exploring others after learning about Principal Component Analysis (PCA)!**</p><p><strong>Task:</strong> Watch and understand the following videos. We recommend taking notes and being able to answer the synthesis questions provided below. Send your I2 teacher/mentor/overlord the answers to the questions over Discord.</p><p>There is a part in the video (20 seconds) that handles some pretty complex math. Feel free to ignore it. The idea is what we want you to learn. Not really the full math.</p><p><strong>Video 1:</strong> <a href="https://www.youtube.com/watch?v=HMOI_lkzW08">StatQuest: PCA main ideas in only 5 minutes!!!</a> <strong>(5 min)</strong></p><p><strong>Video 2:</strong> <a href="https://www.youtube.com/watch?v=FD4DeN81ODY">Principle Component Analysis (PCA)</a> <strong>(5 min)</strong></p><h3 id="synthesis-questions-3"> <a href="#synthesis-questions-3" class="anchor-heading" aria-labelledby="synthesis-questions-3"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <code class="language-plaintext highlighter-rouge">Synthesis Questions:</code></h3><ul><li><code class="language-plaintext highlighter-rouge">What does PCA do?</code><ul><li><code class="language-plaintext highlighter-rouge">Give 3 use cases that you can think of</code></ul><li><code class="language-plaintext highlighter-rouge">What is a principal component?</code></ul><p><strong>Project Spec:</strong></p><p>The project for this “<em>Basics</em>” section will <strong>have you finish a code template on github.</strong> Please ask questions as you work through this project. Be sure to discuss with others in your group if you have one! Share your answers as you like, the goal is to learn and we’re not holding grades over your head.</p><p>This project will be going over k-means clustering (unsupervised ML). We will be using the Scikit-Learn library.</p><p>A few general helpful tips (if applicable):</p><ul><li>Use GitHub, it’s really just better<li>Use <a href="https://www.anaconda.com/">Anaconda</a> with <a href="https://www.python.org/downloads/">Python3</a> in <a href="https://code.visualstudio.com/">VSCode</a>. I personally create .py files but Jupyter Notebooks and Google Colab are also very powerful.<ul><li>For a simple project like this though, powerful computing is unnecessary and you can figure out the details of those other technologies next week<li>If you use Anaconda, create a separate environment so you can mess with libraries and imports all day without screwing up your base environment.</ul><li>Leave comments to cement your understanding. Link syntax to ideas.</ul><p>Check out this handy image that gives popular sk-learn clustering algorithms and their usages:</p><p><img src="../assets/image8.png" alt="alt_text" /></p><p>Also this image visualizing the clustering algorithms:</p><p><img src="../assets/image5.png" alt="alt_text" /></p><p>Read up on k-means clustering in the provided link (Images provided above also contained here). Feel free to check out the other algorithms as well: <a href="https://scikit-learn.org/stable/modules/clustering.html#k-means">SK-Learn Clustering</a></p><p><strong>Clone the Git repo onto your local device if you have not already.</strong></p><p>Then, in your local copy of the GitHub repo, navigate to the unit-1 folder, and work on <strong>clustering-pca.ipynb</strong>. Instructions are in the Jupyter notebook. If you need help setting up your python environment, ask the TA’s!</p><p><strong>GH Link:</strong> <a href="https://github.com/interactive-intelligence/intro-neuro-ai/blob/main/unit-1/clustering-pca.ipynb">Unit 1 Notebook</a> <strong>(30 min)</strong></p><p>When you are finished with your code, independently verify that it works and have fun with it! You could try this method on different datasets, such as <a href="https://www.kaggle.com/datasets/ashwingupta3012/human-faces">this one for example</a>. If you add any additional functionality be sure to talk about it with others and give them ideas.</p><p>Remember that this is all for your learning, so do your best and don’t stress!</p><p>Congratulations! You now understand the basics of Clustering and PCA!</p></div></div><div class="search-overlay"></div></div>
